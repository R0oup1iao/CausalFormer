{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo\n",
    "Demo experiment for quick start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 145515\n",
      "Warning: visualization (Tensorboard) is configured to use, but currently not installed on this machine. Please install TensorboardX with 'pip install tensorboardx', upgrade PyTorch to version >= 1.1 to use 'torch.utils.tensorboard' or turn off the option in the 'config.json' file.\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim15_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 1 time steps.\n",
      "2 causes 0 with a delay of 1 time steps.\n",
      "4 causes 0 with a delay of 1 time steps.\n",
      "1 causes 1 with a delay of 1 time steps.\n",
      "2 causes 1 with a delay of 15 time steps.\n",
      "2 causes 2 with a delay of 1 time steps.\n",
      "3 causes 2 with a delay of 1 time steps.\n",
      "4 causes 2 with a delay of 1 time steps.\n",
      "3 causes 3 with a delay of 1 time steps.\n",
      "4 causes 3 with a delay of 1 time steps.\n",
      "4 causes 4 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 11\n",
      "Total False Negatives: 1\n",
      "Total Direct False Positives: 2\n",
      "Total Direct True Positives: 9\n",
      "TPs': ['0->0', '2->0', '4->0', '1->1', '2->1', '2->2', '3->2', '4->2', '3->3', '4->3', '4->4']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '4->0', '1->1', '2->1', '2->2', '3->2', '3->3', '4->3', '4->4']\n",
      "FPs direct: ['2->0', '4->2']\n",
      "FNs: ['1->0']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.9166666666666666\n",
      "F1' score: 0.9565217391304348\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.8181818181818182\n",
      "Recall: 0.9\n",
      "F1 score: 0.8571428571428572\n",
      "Percentage of delays that are correctly discovered: 88.88888888888889%\n",
      "===================Summary===================\n",
      "\t   Precision'   Recall'       F1'  Precision  Recall        F1       PoD\n",
      "\t1         1.0  0.916667  0.956522   0.818182     0.9  0.857143  0.888889\n"
     ]
    }
   ],
   "source": [
    "! python runner.py -c config/config_fMRI.json -t demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "The following commands are applied sequentially to the dataset:\n",
    "- Diamond\n",
    "- Mediator\n",
    "- V-structure\n",
    "- Fork\n",
    "- Lorenz\n",
    "- fMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 138375\n",
      "Warning: visualization (Tensorboard) is configured to use, but currently not installed on this machine. Please install TensorboardX with 'pip install tensorboardx', upgrade PyTorch to version >= 1.1 to use 'torch.utils.tensorboard' or turn off the option in the 'config.json' file.\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/diamond/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 1 time steps.\n",
      "V2 causes V2 with a delay of 2 time steps.\n",
      "V2 causes V3 with a delay of 0 time steps.\n",
      "V4 causes V4 with a delay of 12 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 1\n",
      "Total True Positives': 3\n",
      "Total False Negatives: 5\n",
      "Total Direct False Positives: 1\n",
      "Total Direct True Positives: 3\n",
      "TPs': ['0->0', '1->1', '3->3']\n",
      "FPs': ['1->2']\n",
      "TPs direct: ['0->0', '1->1', '3->3']\n",
      "FPs direct: ['1->2']\n",
      "FNs: ['0->1', '0->2', '2->2', '1->3', '2->3']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.75\n",
      "Recall': 0.375\n",
      "F1' score: 0.5\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.75\n",
      "Recall: 0.375\n",
      "F1 score: 0.5\n",
      "Percentage of delays that are correctly discovered: 33.33333333333333%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 138375\n",
      "Warning: visualization (Tensorboard) is configured to use, but currently not installed on this machine. Please install TensorboardX with 'pip install tensorboardx', upgrade PyTorch to version >= 1.1 to use 'torch.utils.tensorboard' or turn off the option in the 'config.json' file.\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/diamond/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 1 time steps.\n",
      "V2 causes V2 with a delay of 1 time steps.\n",
      "V3 causes V3 with a delay of 1 time steps.\n",
      "V4 causes V4 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 4\n",
      "Total False Negatives: 4\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 4\n",
      "TPs': ['0->0', '1->1', '2->2', '3->3']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->2', '3->3']\n",
      "FPs direct: []\n",
      "FNs: ['0->1', '0->2', '1->3', '2->3']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.5\n",
      "F1' score: 0.6666666666666666\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.5\n",
      "F1 score: 0.6666666666666666\n",
      "Percentage of delays that are correctly discovered: 100.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 138375\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/diamond/groundtruth.csv\n",
      "===================Results===================\n",
      "V2 causes V1 with a delay of 2 time steps.\n",
      "V2 causes V2 with a delay of 12 time steps.\n",
      "V4 causes V2 with a delay of 12 time steps.\n",
      "V1 causes V3 with a delay of 0 time steps.\n",
      "V3 causes V3 with a delay of 2 time steps.\n",
      "V3 causes V4 with a delay of 0 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 2\n",
      "Total True Positives': 4\n",
      "Total False Negatives: 4\n",
      "Total Direct False Positives: 2\n",
      "Total Direct True Positives: 4\n",
      "TPs': ['1->1', '0->2', '2->2', '2->3']\n",
      "FPs': ['1->0', '3->1']\n",
      "TPs direct: ['1->1', '0->2', '2->2', '2->3']\n",
      "FPs direct: ['1->0', '3->1']\n",
      "FNs: ['0->0', '0->1', '1->3', '3->3']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.6666666666666666\n",
      "Recall': 0.5\n",
      "F1' score: 0.5714285714285715\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.5\n",
      "F1 score: 0.5714285714285715\n",
      "Percentage of delays that are correctly discovered: 0.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 138375\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/diamond/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 1 time steps.\n",
      "V2 causes V2 with a delay of 1 time steps.\n",
      "V3 causes V3 with a delay of 1 time steps.\n",
      "V4 causes V4 with a delay of 8 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 4\n",
      "Total False Negatives: 4\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 4\n",
      "TPs': ['0->0', '1->1', '2->2', '3->3']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->2', '3->3']\n",
      "FPs direct: []\n",
      "FNs: ['0->1', '0->2', '1->3', '2->3']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.5\n",
      "F1' score: 0.6666666666666666\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.5\n",
      "F1 score: 0.6666666666666666\n",
      "Percentage of delays that are correctly discovered: 75.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 138375\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/diamond/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 1 time steps.\n",
      "V2 causes V2 with a delay of 1 time steps.\n",
      "V3 causes V3 with a delay of 1 time steps.\n",
      "V4 causes V4 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 4\n",
      "Total False Negatives: 4\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 4\n",
      "TPs': ['0->0', '1->1', '2->2', '3->3']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->2', '3->3']\n",
      "FPs direct: []\n",
      "FNs: ['0->1', '0->2', '1->3', '2->3']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.5\n",
      "F1' score: 0.6666666666666666\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.5\n",
      "F1 score: 0.6666666666666666\n",
      "Percentage of delays that are correctly discovered: 100.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 138375\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/diamond/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 1 time steps.\n",
      "V2 causes V2 with a delay of 1 time steps.\n",
      "V3 causes V3 with a delay of 1 time steps.\n",
      "V1 causes V4 with a delay of 5 time steps.\n",
      "V4 causes V4 with a delay of 5 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 5\n",
      "Total False Negatives: 4\n",
      "Total Direct False Positives: 1\n",
      "Total Direct True Positives: 4\n",
      "TPs': ['0->0', '1->1', '2->2', '0->3', '3->3']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->2', '3->3']\n",
      "FPs direct: ['0->3']\n",
      "FNs: ['0->1', '0->2', '1->3', '2->3']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.5555555555555556\n",
      "F1' score: 0.7142857142857143\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.8\n",
      "Recall: 0.5\n",
      "F1 score: 0.6153846153846154\n",
      "Percentage of delays that are correctly discovered: 75.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 138375\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/diamond/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 1 time steps.\n",
      "V1 causes V2 with a delay of 6 time steps.\n",
      "V2 causes V2 with a delay of 1 time steps.\n",
      "V3 causes V3 with a delay of 2 time steps.\n",
      "V4 causes V3 with a delay of 0 time steps.\n",
      "V4 causes V4 with a delay of 2 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 1\n",
      "Total True Positives': 5\n",
      "Total False Negatives: 3\n",
      "Total Direct False Positives: 1\n",
      "Total Direct True Positives: 5\n",
      "TPs': ['0->0', '0->1', '1->1', '2->2', '3->3']\n",
      "FPs': ['3->2']\n",
      "TPs direct: ['0->0', '0->1', '1->1', '2->2', '3->3']\n",
      "FPs direct: ['3->2']\n",
      "FNs: ['0->2', '1->3', '2->3']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.8333333333333334\n",
      "Recall': 0.625\n",
      "F1' score: 0.7142857142857143\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.8333333333333334\n",
      "Recall: 0.625\n",
      "F1 score: 0.7142857142857143\n",
      "Percentage of delays that are correctly discovered: 40.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 138375\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/diamond/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 1 time steps.\n",
      "V2 causes V2 with a delay of 1 time steps.\n",
      "V1 causes V3 with a delay of 1 time steps.\n",
      "V4 causes V4 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 4\n",
      "Total False Negatives: 4\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 4\n",
      "TPs': ['0->0', '1->1', '0->2', '3->3']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '0->2', '3->3']\n",
      "FPs direct: []\n",
      "FNs: ['0->1', '2->2', '1->3', '2->3']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.5\n",
      "F1' score: 0.6666666666666666\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.5\n",
      "F1 score: 0.6666666666666666\n",
      "Percentage of delays that are correctly discovered: 100.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 138375\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/diamond/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 1 time steps.\n",
      "V2 causes V2 with a delay of 1 time steps.\n",
      "V2 causes V3 with a delay of 0 time steps.\n",
      "V4 causes V4 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 1\n",
      "Total True Positives': 3\n",
      "Total False Negatives: 5\n",
      "Total Direct False Positives: 1\n",
      "Total Direct True Positives: 3\n",
      "TPs': ['0->0', '1->1', '3->3']\n",
      "FPs': ['1->2']\n",
      "TPs direct: ['0->0', '1->1', '3->3']\n",
      "FPs direct: ['1->2']\n",
      "FNs: ['0->1', '0->2', '2->2', '1->3', '2->3']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.75\n",
      "Recall': 0.375\n",
      "F1' score: 0.5\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.75\n",
      "Recall: 0.375\n",
      "F1 score: 0.5\n",
      "Percentage of delays that are correctly discovered: 100.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 138375\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/diamond/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 1 time steps.\n",
      "V2 causes V2 with a delay of 1 time steps.\n",
      "V3 causes V3 with a delay of 1 time steps.\n",
      "V2 causes V4 with a delay of 1 time steps.\n",
      "V4 causes V4 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 5\n",
      "Total False Negatives: 3\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 5\n",
      "TPs': ['0->0', '1->1', '2->2', '1->3', '3->3']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->2', '1->3', '3->3']\n",
      "FPs direct: []\n",
      "FNs: ['0->1', '0->2', '2->3']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.625\n",
      "F1' score: 0.7692307692307693\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.625\n",
      "F1 score: 0.7692307692307693\n",
      "Percentage of delays that are correctly discovered: 100.0%\n",
      "===================Summary===================\n",
      "\t    Precision'   Recall'       F1'  Precision  Recall        F1       PoD\n",
      "\t1     0.750000  0.375000  0.500000   0.750000   0.375  0.500000  0.333333\n",
      "\t2     1.000000  0.500000  0.666667   1.000000   0.500  0.666667  1.000000\n",
      "\t3     0.666667  0.500000  0.571429   0.666667   0.500  0.571429  0.000000\n",
      "\t4     1.000000  0.500000  0.666667   1.000000   0.500  0.666667  0.750000\n",
      "\t5     1.000000  0.500000  0.666667   1.000000   0.500  0.666667  1.000000\n",
      "\t6     1.000000  0.555556  0.714286   0.800000   0.500  0.615385  0.750000\n",
      "\t7     0.833333  0.625000  0.714286   0.833333   0.625  0.714286  0.400000\n",
      "\t8     1.000000  0.500000  0.666667   1.000000   0.500  0.666667  1.000000\n",
      "\t9     0.750000  0.375000  0.500000   0.750000   0.375  0.500000  1.000000\n",
      "\t10    1.000000  0.625000  0.769231   1.000000   0.625  0.769231  1.000000\n"
     ]
    }
   ],
   "source": [
    "! python runner.py -c config/config_basic_diamond_mediator.json -t diamond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/mediator/groundtruth.csv\n",
      "===================Results===================\n",
      "V2 causes V1 with a delay of 6 time steps.\n",
      "V2 causes V2 with a delay of 1 time steps.\n",
      "V3 causes V2 with a delay of 13 time steps.\n",
      "V3 causes V3 with a delay of 2 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 2\n",
      "Total True Positives': 2\n",
      "Total False Negatives: 4\n",
      "Total Direct False Positives: 2\n",
      "Total Direct True Positives: 2\n",
      "TPs': ['1->1', '2->2']\n",
      "FPs': ['1->0', '2->1']\n",
      "TPs direct: ['1->1', '2->2']\n",
      "FPs direct: ['1->0', '2->1']\n",
      "FNs: ['0->0', '0->1', '0->2', '1->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.5\n",
      "Recall': 0.3333333333333333\n",
      "F1' score: 0.4\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.5\n",
      "Recall: 0.3333333333333333\n",
      "F1 score: 0.4\n",
      "Percentage of delays that are correctly discovered: 50.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/mediator/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 1 time steps.\n",
      "V2 causes V2 with a delay of 1 time steps.\n",
      "V3 causes V3 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 3\n",
      "Total False Negatives: 3\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 3\n",
      "TPs': ['0->0', '1->1', '2->2']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->2']\n",
      "FPs direct: []\n",
      "FNs: ['0->1', '0->2', '1->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.5\n",
      "F1' score: 0.6666666666666666\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.5\n",
      "F1 score: 0.6666666666666666\n",
      "Percentage of delays that are correctly discovered: 100.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/mediator/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 1 time steps.\n",
      "V2 causes V2 with a delay of 1 time steps.\n",
      "V3 causes V3 with a delay of 2 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 3\n",
      "Total False Negatives: 3\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 3\n",
      "TPs': ['0->0', '1->1', '2->2']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->2']\n",
      "FPs direct: []\n",
      "FNs: ['0->1', '0->2', '1->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.5\n",
      "F1' score: 0.6666666666666666\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.5\n",
      "F1 score: 0.6666666666666666\n",
      "Percentage of delays that are correctly discovered: 66.66666666666666%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/mediator/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 1 time steps.\n",
      "V1 causes V2 with a delay of 1 time steps.\n",
      "V2 causes V3 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 3\n",
      "Total False Negatives: 3\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 3\n",
      "TPs': ['0->0', '0->1', '1->2']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '0->1', '1->2']\n",
      "FPs direct: []\n",
      "FNs: ['1->1', '0->2', '2->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.5\n",
      "F1' score: 0.6666666666666666\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.5\n",
      "F1 score: 0.6666666666666666\n",
      "Percentage of delays that are correctly discovered: 100.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/mediator/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 2 time steps.\n",
      "V1 causes V2 with a delay of 2 time steps.\n",
      "V2 causes V2 with a delay of 2 time steps.\n",
      "V3 causes V3 with a delay of 2 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 4\n",
      "Total False Negatives: 2\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 4\n",
      "TPs': ['0->0', '0->1', '1->1', '2->2']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '0->1', '1->1', '2->2']\n",
      "FPs direct: []\n",
      "FNs: ['0->2', '1->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.6666666666666666\n",
      "F1' score: 0.8\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.6666666666666666\n",
      "F1 score: 0.8\n",
      "Percentage of delays that are correctly discovered: 0.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/mediator/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 1 time steps.\n",
      "V2 causes V2 with a delay of 1 time steps.\n",
      "V2 causes V3 with a delay of 14 time steps.\n",
      "V3 causes V3 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 4\n",
      "Total False Negatives: 2\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 4\n",
      "TPs': ['0->0', '1->1', '1->2', '2->2']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '1->2', '2->2']\n",
      "FPs direct: []\n",
      "FNs: ['0->1', '0->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.6666666666666666\n",
      "F1' score: 0.8\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.6666666666666666\n",
      "F1 score: 0.8\n",
      "Percentage of delays that are correctly discovered: 75.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/mediator/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 1 time steps.\n",
      "V2 causes V2 with a delay of 1 time steps.\n",
      "V3 causes V3 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 3\n",
      "Total False Negatives: 3\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 3\n",
      "TPs': ['0->0', '1->1', '2->2']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->2']\n",
      "FPs direct: []\n",
      "FNs: ['0->1', '0->2', '1->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.5\n",
      "F1' score: 0.6666666666666666\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.5\n",
      "F1 score: 0.6666666666666666\n",
      "Percentage of delays that are correctly discovered: 100.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/mediator/groundtruth.csv\n",
      "===================Results===================\n",
      "V2 causes V1 with a delay of 4 time steps.\n",
      "V3 causes V1 with a delay of 0 time steps.\n",
      "V2 causes V2 with a delay of 2 time steps.\n",
      "V3 causes V3 with a delay of 2 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 2\n",
      "Total True Positives': 2\n",
      "Total False Negatives: 4\n",
      "Total Direct False Positives: 2\n",
      "Total Direct True Positives: 2\n",
      "TPs': ['1->1', '2->2']\n",
      "FPs': ['1->0', '2->0']\n",
      "TPs direct: ['1->1', '2->2']\n",
      "FPs direct: ['1->0', '2->0']\n",
      "FNs: ['0->0', '0->1', '0->2', '1->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.5\n",
      "Recall': 0.3333333333333333\n",
      "F1' score: 0.4\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.5\n",
      "Recall: 0.3333333333333333\n",
      "F1 score: 0.4\n",
      "Percentage of delays that are correctly discovered: 0.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/mediator/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 1 time steps.\n",
      "V1 causes V2 with a delay of 1 time steps.\n",
      "V2 causes V2 with a delay of 7 time steps.\n",
      "V2 causes V3 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 4\n",
      "Total False Negatives: 2\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 4\n",
      "TPs': ['0->0', '0->1', '1->1', '1->2']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '0->1', '1->1', '1->2']\n",
      "FPs direct: []\n",
      "FNs: ['0->2', '2->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.6666666666666666\n",
      "F1' score: 0.8\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.6666666666666666\n",
      "F1 score: 0.8\n",
      "Percentage of delays that are correctly discovered: 75.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/mediator/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 1 time steps.\n",
      "V2 causes V2 with a delay of 1 time steps.\n",
      "V3 causes V3 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 3\n",
      "Total False Negatives: 3\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 3\n",
      "TPs': ['0->0', '1->1', '2->2']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->2']\n",
      "FPs direct: []\n",
      "FNs: ['0->1', '0->2', '1->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.5\n",
      "F1' score: 0.6666666666666666\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.5\n",
      "F1 score: 0.6666666666666666\n",
      "Percentage of delays that are correctly discovered: 100.0%\n",
      "===================Summary===================\n",
      "\t    Precision'   Recall'       F1'  Precision    Recall        F1       PoD\n",
      "\t1          0.5  0.333333  0.400000        0.5  0.333333  0.400000  0.500000\n",
      "\t2          1.0  0.500000  0.666667        1.0  0.500000  0.666667  1.000000\n",
      "\t3          1.0  0.500000  0.666667        1.0  0.500000  0.666667  0.666667\n",
      "\t4          1.0  0.500000  0.666667        1.0  0.500000  0.666667  1.000000\n",
      "\t5          1.0  0.666667  0.800000        1.0  0.666667  0.800000  0.000000\n",
      "\t6          1.0  0.666667  0.800000        1.0  0.666667  0.800000  0.750000\n",
      "\t7          1.0  0.500000  0.666667        1.0  0.500000  0.666667  1.000000\n",
      "\t8          0.5  0.333333  0.400000        0.5  0.333333  0.400000  0.000000\n",
      "\t9          1.0  0.666667  0.800000        1.0  0.666667  0.800000  0.750000\n",
      "\t10         1.0  0.500000  0.666667        1.0  0.500000  0.666667  1.000000\n"
     ]
    }
   ],
   "source": [
    "! python runner.py -c config/config_basic_diamond_mediator.json -t mediator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/v/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 1 time steps.\n",
      "V2 causes V2 with a delay of 1 time steps.\n",
      "V3 causes V3 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 3\n",
      "Total False Negatives: 2\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 3\n",
      "TPs': ['0->0', '1->1', '2->2']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->2']\n",
      "FPs direct: []\n",
      "FNs: ['0->2', '1->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.6\n",
      "F1' score: 0.7499999999999999\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.6\n",
      "F1 score: 0.7499999999999999\n",
      "Percentage of delays that are correctly discovered: 100.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/v/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 5 time steps.\n",
      "V2 causes V2 with a delay of 1 time steps.\n",
      "V3 causes V2 with a delay of 2 time steps.\n",
      "V2 causes V3 with a delay of 2 time steps.\n",
      "V3 causes V3 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 1\n",
      "Total True Positives': 4\n",
      "Total False Negatives: 1\n",
      "Total Direct False Positives: 1\n",
      "Total Direct True Positives: 4\n",
      "TPs': ['0->0', '1->1', '1->2', '2->2']\n",
      "FPs': ['2->1']\n",
      "TPs direct: ['0->0', '1->1', '1->2', '2->2']\n",
      "FPs direct: ['2->1']\n",
      "FNs: ['0->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.8\n",
      "Recall': 0.8\n",
      "F1' score: 0.8000000000000002\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.8\n",
      "Recall: 0.8\n",
      "F1 score: 0.8000000000000002\n",
      "Percentage of delays that are correctly discovered: 50.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/v/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 1 time steps.\n",
      "V2 causes V2 with a delay of 1 time steps.\n",
      "V3 causes V3 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 3\n",
      "Total False Negatives: 2\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 3\n",
      "TPs': ['0->0', '1->1', '2->2']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->2']\n",
      "FPs direct: []\n",
      "FNs: ['0->2', '1->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.6\n",
      "F1' score: 0.7499999999999999\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.6\n",
      "F1 score: 0.7499999999999999\n",
      "Percentage of delays that are correctly discovered: 100.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/v/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 8 time steps.\n",
      "V1 causes V2 with a delay of 0 time steps.\n",
      "V3 causes V2 with a delay of 9 time steps.\n",
      "V1 causes V3 with a delay of 4 time steps.\n",
      "V3 causes V3 with a delay of 4 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 2\n",
      "Total True Positives': 3\n",
      "Total False Negatives: 2\n",
      "Total Direct False Positives: 2\n",
      "Total Direct True Positives: 3\n",
      "TPs': ['0->0', '0->2', '2->2']\n",
      "FPs': ['0->1', '2->1']\n",
      "TPs direct: ['0->0', '0->2', '2->2']\n",
      "FPs direct: ['0->1', '2->1']\n",
      "FNs: ['1->1', '1->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.6\n",
      "Recall': 0.6\n",
      "F1' score: 0.6\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.6\n",
      "Recall: 0.6\n",
      "F1 score: 0.6\n",
      "Percentage of delays that are correctly discovered: 0.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/v/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 5 time steps.\n",
      "V2 causes V2 with a delay of 1 time steps.\n",
      "V3 causes V3 with a delay of 5 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 3\n",
      "Total False Negatives: 2\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 3\n",
      "TPs': ['0->0', '1->1', '2->2']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->2']\n",
      "FPs direct: []\n",
      "FNs: ['0->2', '1->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.6\n",
      "F1' score: 0.7499999999999999\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.6\n",
      "F1 score: 0.7499999999999999\n",
      "Percentage of delays that are correctly discovered: 33.33333333333333%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/v/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 1 time steps.\n",
      "V1 causes V2 with a delay of 15 time steps.\n",
      "V2 causes V2 with a delay of 1 time steps.\n",
      "V2 causes V3 with a delay of 4 time steps.\n",
      "V3 causes V3 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 1\n",
      "Total True Positives': 4\n",
      "Total False Negatives: 1\n",
      "Total Direct False Positives: 1\n",
      "Total Direct True Positives: 4\n",
      "TPs': ['0->0', '1->1', '1->2', '2->2']\n",
      "FPs': ['0->1']\n",
      "TPs direct: ['0->0', '1->1', '1->2', '2->2']\n",
      "FPs direct: ['0->1']\n",
      "FNs: ['0->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.8\n",
      "Recall': 0.8\n",
      "F1' score: 0.8000000000000002\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.8\n",
      "Recall: 0.8\n",
      "F1 score: 0.8000000000000002\n",
      "Percentage of delays that are correctly discovered: 75.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/v/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 6 time steps.\n",
      "V2 causes V2 with a delay of 6 time steps.\n",
      "V1 causes V3 with a delay of 2 time steps.\n",
      "V3 causes V3 with a delay of 8 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 4\n",
      "Total False Negatives: 1\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 4\n",
      "TPs': ['0->0', '1->1', '0->2', '2->2']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '0->2', '2->2']\n",
      "FPs direct: []\n",
      "FNs: ['1->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.8\n",
      "F1' score: 0.888888888888889\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.8\n",
      "F1 score: 0.888888888888889\n",
      "Percentage of delays that are correctly discovered: 0.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/v/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 3 time steps.\n",
      "V1 causes V2 with a delay of 1 time steps.\n",
      "V2 causes V2 with a delay of 5 time steps.\n",
      "V1 causes V3 with a delay of 4 time steps.\n",
      "V3 causes V3 with a delay of 4 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 1\n",
      "Total True Positives': 4\n",
      "Total False Negatives: 1\n",
      "Total Direct False Positives: 1\n",
      "Total Direct True Positives: 4\n",
      "TPs': ['0->0', '1->1', '0->2', '2->2']\n",
      "FPs': ['0->1']\n",
      "TPs direct: ['0->0', '1->1', '0->2', '2->2']\n",
      "FPs direct: ['0->1']\n",
      "FNs: ['1->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.8\n",
      "Recall': 0.8\n",
      "F1' score: 0.8000000000000002\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.8\n",
      "Recall: 0.8\n",
      "F1 score: 0.8000000000000002\n",
      "Percentage of delays that are correctly discovered: 0.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/v/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 12 time steps.\n",
      "V2 causes V2 with a delay of 12 time steps.\n",
      "V1 causes V3 with a delay of 2 time steps.\n",
      "V2 causes V3 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 4\n",
      "Total False Negatives: 1\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 4\n",
      "TPs': ['0->0', '1->1', '0->2', '1->2']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '0->2', '1->2']\n",
      "FPs direct: []\n",
      "FNs: ['2->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.8\n",
      "F1' score: 0.888888888888889\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.8\n",
      "F1 score: 0.888888888888889\n",
      "Percentage of delays that are correctly discovered: 33.33333333333333%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/v/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 1 time steps.\n",
      "V2 causes V2 with a delay of 1 time steps.\n",
      "V3 causes V3 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 3\n",
      "Total False Negatives: 2\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 3\n",
      "TPs': ['0->0', '1->1', '2->2']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->2']\n",
      "FPs direct: []\n",
      "FNs: ['0->2', '1->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.6\n",
      "F1' score: 0.7499999999999999\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.6\n",
      "F1 score: 0.7499999999999999\n",
      "Percentage of delays that are correctly discovered: 100.0%\n",
      "===================Summary===================\n",
      "\t    Precision'  Recall'       F1'  Precision  Recall        F1       PoD\n",
      "\t1          1.0      0.6  0.750000        1.0     0.6  0.750000  1.000000\n",
      "\t2          0.8      0.8  0.800000        0.8     0.8  0.800000  0.500000\n",
      "\t3          1.0      0.6  0.750000        1.0     0.6  0.750000  1.000000\n",
      "\t4          0.6      0.6  0.600000        0.6     0.6  0.600000  0.000000\n",
      "\t5          1.0      0.6  0.750000        1.0     0.6  0.750000  0.333333\n",
      "\t6          0.8      0.8  0.800000        0.8     0.8  0.800000  0.750000\n",
      "\t7          1.0      0.8  0.888889        1.0     0.8  0.888889  0.000000\n",
      "\t8          0.8      0.8  0.800000        0.8     0.8  0.800000  0.000000\n",
      "\t9          1.0      0.8  0.888889        1.0     0.8  0.888889  0.333333\n",
      "\t10         1.0      0.6  0.750000        1.0     0.6  0.750000  1.000000\n"
     ]
    }
   ],
   "source": [
    "! python runner.py -c config/config_basic_v_fork.json -t v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/fork/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 8 time steps.\n",
      "V2 causes V2 with a delay of 5 time steps.\n",
      "V3 causes V3 with a delay of 8 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 3\n",
      "Total False Negatives: 2\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 3\n",
      "TPs': ['0->0', '1->1', '2->2']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->2']\n",
      "FPs direct: []\n",
      "FNs: ['0->1', '0->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.6\n",
      "F1' score: 0.7499999999999999\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.6\n",
      "F1 score: 0.7499999999999999\n",
      "Percentage of delays that are correctly discovered: 0.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/fork/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 1 time steps.\n",
      "V2 causes V2 with a delay of 1 time steps.\n",
      "V3 causes V3 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 3\n",
      "Total False Negatives: 2\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 3\n",
      "TPs': ['0->0', '1->1', '2->2']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->2']\n",
      "FPs direct: []\n",
      "FNs: ['0->1', '0->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.6\n",
      "F1' score: 0.7499999999999999\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.6\n",
      "F1 score: 0.7499999999999999\n",
      "Percentage of delays that are correctly discovered: 100.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/fork/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 1 time steps.\n",
      "V1 causes V2 with a delay of 12 time steps.\n",
      "V2 causes V2 with a delay of 1 time steps.\n",
      "V2 causes V3 with a delay of 13 time steps.\n",
      "V3 causes V3 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 1\n",
      "Total True Positives': 4\n",
      "Total False Negatives: 1\n",
      "Total Direct False Positives: 1\n",
      "Total Direct True Positives: 4\n",
      "TPs': ['0->0', '0->1', '1->1', '2->2']\n",
      "FPs': ['1->2']\n",
      "TPs direct: ['0->0', '0->1', '1->1', '2->2']\n",
      "FPs direct: ['1->2']\n",
      "FNs: ['0->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.8\n",
      "Recall': 0.8\n",
      "F1' score: 0.8000000000000002\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.8\n",
      "Recall: 0.8\n",
      "F1 score: 0.8000000000000002\n",
      "Percentage of delays that are correctly discovered: 75.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/fork/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 14 time steps.\n",
      "V2 causes V2 with a delay of 14 time steps.\n",
      "V3 causes V2 with a delay of 2 time steps.\n",
      "V3 causes V3 with a delay of 14 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 1\n",
      "Total True Positives': 3\n",
      "Total False Negatives: 2\n",
      "Total Direct False Positives: 1\n",
      "Total Direct True Positives: 3\n",
      "TPs': ['0->0', '1->1', '2->2']\n",
      "FPs': ['2->1']\n",
      "TPs direct: ['0->0', '1->1', '2->2']\n",
      "FPs direct: ['2->1']\n",
      "FNs: ['0->1', '0->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.75\n",
      "Recall': 0.6\n",
      "F1' score: 0.6666666666666665\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.75\n",
      "Recall: 0.6\n",
      "F1 score: 0.6666666666666665\n",
      "Percentage of delays that are correctly discovered: 0.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/fork/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 5 time steps.\n",
      "V1 causes V2 with a delay of 1 time steps.\n",
      "V2 causes V2 with a delay of 5 time steps.\n",
      "V3 causes V3 with a delay of 5 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 4\n",
      "Total False Negatives: 1\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 4\n",
      "TPs': ['0->0', '0->1', '1->1', '2->2']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '0->1', '1->1', '2->2']\n",
      "FPs direct: []\n",
      "FNs: ['0->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.8\n",
      "F1' score: 0.888888888888889\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.8\n",
      "F1 score: 0.888888888888889\n",
      "Percentage of delays that are correctly discovered: 25.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/fork/groundtruth.csv\n",
      "===================Results===================\n",
      "V2 causes V1 with a delay of 6 time steps.\n",
      "V1 causes V2 with a delay of 4 time steps.\n",
      "V1 causes V3 with a delay of 6 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 1\n",
      "Total True Positives': 2\n",
      "Total False Negatives: 3\n",
      "Total Direct False Positives: 1\n",
      "Total Direct True Positives: 2\n",
      "TPs': ['0->1', '0->2']\n",
      "FPs': ['1->0']\n",
      "TPs direct: ['0->1', '0->2']\n",
      "FPs direct: ['1->0']\n",
      "FNs: ['0->0', '1->1', '2->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.6666666666666666\n",
      "Recall': 0.4\n",
      "F1' score: 0.5\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.4\n",
      "F1 score: 0.5\n",
      "Percentage of delays that are correctly discovered: 0.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/fork/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 1 time steps.\n",
      "V2 causes V2 with a delay of 4 time steps.\n",
      "V3 causes V3 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 3\n",
      "Total False Negatives: 2\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 3\n",
      "TPs': ['0->0', '1->1', '2->2']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->2']\n",
      "FPs direct: []\n",
      "FNs: ['0->1', '0->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.6\n",
      "F1' score: 0.7499999999999999\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.6\n",
      "F1 score: 0.7499999999999999\n",
      "Percentage of delays that are correctly discovered: 66.66666666666666%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/fork/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 4 time steps.\n",
      "V2 causes V2 with a delay of 7 time steps.\n",
      "V2 causes V3 with a delay of 2 time steps.\n",
      "V3 causes V3 with a delay of 7 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 1\n",
      "Total True Positives': 3\n",
      "Total False Negatives: 2\n",
      "Total Direct False Positives: 1\n",
      "Total Direct True Positives: 3\n",
      "TPs': ['0->0', '1->1', '2->2']\n",
      "FPs': ['1->2']\n",
      "TPs direct: ['0->0', '1->1', '2->2']\n",
      "FPs direct: ['1->2']\n",
      "FNs: ['0->1', '0->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.75\n",
      "Recall': 0.6\n",
      "F1' score: 0.6666666666666665\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.75\n",
      "Recall: 0.6\n",
      "F1 score: 0.6666666666666665\n",
      "Percentage of delays that are correctly discovered: 0.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/fork/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 9 time steps.\n",
      "V2 causes V1 with a delay of 1 time steps.\n",
      "V1 causes V2 with a delay of 1 time steps.\n",
      "V2 causes V2 with a delay of 9 time steps.\n",
      "V3 causes V3 with a delay of 9 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 1\n",
      "Total True Positives': 4\n",
      "Total False Negatives: 1\n",
      "Total Direct False Positives: 1\n",
      "Total Direct True Positives: 4\n",
      "TPs': ['0->0', '0->1', '1->1', '2->2']\n",
      "FPs': ['1->0']\n",
      "TPs direct: ['0->0', '0->1', '1->1', '2->2']\n",
      "FPs direct: ['1->0']\n",
      "FNs: ['0->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.8\n",
      "Recall': 0.8\n",
      "F1' score: 0.8000000000000002\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.8\n",
      "Recall: 0.8\n",
      "F1 score: 0.8000000000000002\n",
      "Percentage of delays that are correctly discovered: 25.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=16, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((16, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 137899\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/basic/fork/groundtruth.csv\n",
      "===================Results===================\n",
      "V1 causes V1 with a delay of 13 time steps.\n",
      "V2 causes V2 with a delay of 8 time steps.\n",
      "V3 causes V2 with a delay of 2 time steps.\n",
      "V3 causes V3 with a delay of 9 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 1\n",
      "Total True Positives': 3\n",
      "Total False Negatives: 2\n",
      "Total Direct False Positives: 1\n",
      "Total Direct True Positives: 3\n",
      "TPs': ['0->0', '1->1', '2->2']\n",
      "FPs': ['2->1']\n",
      "TPs direct: ['0->0', '1->1', '2->2']\n",
      "FPs direct: ['2->1']\n",
      "FNs: ['0->1', '0->2']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.75\n",
      "Recall': 0.6\n",
      "F1' score: 0.6666666666666665\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.75\n",
      "Recall: 0.6\n",
      "F1 score: 0.6666666666666665\n",
      "Percentage of delays that are correctly discovered: 0.0%\n",
      "===================Summary===================\n",
      "\t    Precision'  Recall'       F1'  Precision  Recall        F1       PoD\n",
      "\t1     1.000000      0.6  0.750000   1.000000     0.6  0.750000  0.000000\n",
      "\t2     1.000000      0.6  0.750000   1.000000     0.6  0.750000  1.000000\n",
      "\t3     0.800000      0.8  0.800000   0.800000     0.8  0.800000  0.750000\n",
      "\t4     0.750000      0.6  0.666667   0.750000     0.6  0.666667  0.000000\n",
      "\t5     1.000000      0.8  0.888889   1.000000     0.8  0.888889  0.250000\n",
      "\t6     0.666667      0.4  0.500000   0.666667     0.4  0.500000  0.000000\n",
      "\t7     1.000000      0.6  0.750000   1.000000     0.6  0.750000  0.666667\n",
      "\t8     0.750000      0.6  0.666667   0.750000     0.6  0.666667  0.000000\n",
      "\t9     0.800000      0.8  0.800000   0.800000     0.8  0.800000  0.250000\n",
      "\t10    0.750000      0.6  0.666667   0.750000     0.6  0.666667  0.000000\n"
     ]
    }
   ],
   "source": [
    "! python runner.py -c config/config_basic_v_fork.json -t fork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=512, bias=True)\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=8, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 571307\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/lorenz96/groundtruth.csv\n",
      "===================Results===================\n",
      "T0 causes T0 with a delay of 1 time steps.\n",
      "T2 causes T0 with a delay of 0 time steps.\n",
      "T8 causes T0 with a delay of 1 time steps.\n",
      "T1 causes T1 with a delay of 1 time steps.\n",
      "T3 causes T1 with a delay of 0 time steps.\n",
      "T9 causes T1 with a delay of 1 time steps.\n",
      "T0 causes T2 with a delay of 0 time steps.\n",
      "T2 causes T2 with a delay of 1 time steps.\n",
      "T1 causes T3 with a delay of 0 time steps.\n",
      "T3 causes T3 with a delay of 2 time steps.\n",
      "T2 causes T4 with a delay of 1 time steps.\n",
      "T4 causes T4 with a delay of 1 time steps.\n",
      "T3 causes T5 with a delay of 0 time steps.\n",
      "T5 causes T5 with a delay of 2 time steps.\n",
      "T4 causes T6 with a delay of 1 time steps.\n",
      "T6 causes T6 with a delay of 1 time steps.\n",
      "T8 causes T6 with a delay of 0 time steps.\n",
      "T5 causes T7 with a delay of 1 time steps.\n",
      "T7 causes T7 with a delay of 1 time steps.\n",
      "T0 causes T8 with a delay of 0 time steps.\n",
      "T6 causes T8 with a delay of 1 time steps.\n",
      "T8 causes T8 with a delay of 1 time steps.\n",
      "T7 causes T9 with a delay of 1 time steps.\n",
      "T9 causes T9 with a delay of 2 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 24\n",
      "Total False Negatives: 20\n",
      "Total Direct False Positives: 4\n",
      "Total Direct True Positives: 20\n",
      "TPs': ['0->0', '2->0', '8->0', '1->1', '3->1', '9->1', '0->2', '2->2', '1->3', '3->3', '2->4', '4->4', '3->5', '5->5', '4->6', '6->6', '8->6', '5->7', '7->7', '0->8', '6->8', '8->8', '7->9', '9->9']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '8->0', '1->1', '9->1', '0->2', '2->2', '1->3', '3->3', '2->4', '4->4', '3->5', '5->5', '4->6', '6->6', '5->7', '7->7', '6->8', '8->8', '7->9', '9->9']\n",
      "FPs direct: ['2->0', '3->1', '8->6', '0->8']\n",
      "FNs: ['1->0', '9->0', '0->1', '2->1', '1->2', '3->2', '2->3', '4->3', '3->4', '5->4', '4->5', '6->5', '5->6', '7->6', '6->7', '8->7', '7->8', '9->8', '0->9', '8->9']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.5454545454545454\n",
      "F1' score: 0.7058823529411764\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.8333333333333334\n",
      "Recall: 0.5\n",
      "F1 score: 0.625\n",
      "Percentage of delays that are correctly discovered: 70.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=512, bias=True)\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=8, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 571307\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/lorenz96/groundtruth.csv\n",
      "===================Results===================\n",
      "T0 causes T0 with a delay of 1 time steps.\n",
      "T1 causes T0 with a delay of 1 time steps.\n",
      "T8 causes T0 with a delay of 2 time steps.\n",
      "T1 causes T1 with a delay of 1 time steps.\n",
      "T9 causes T1 with a delay of 5 time steps.\n",
      "T0 causes T2 with a delay of 5 time steps.\n",
      "T2 causes T2 with a delay of 1 time steps.\n",
      "T3 causes T2 with a delay of 1 time steps.\n",
      "T1 causes T3 with a delay of 2 time steps.\n",
      "T3 causes T3 with a delay of 1 time steps.\n",
      "T4 causes T3 with a delay of 1 time steps.\n",
      "T1 causes T4 with a delay of 3 time steps.\n",
      "T2 causes T4 with a delay of 10 time steps.\n",
      "T4 causes T4 with a delay of 1 time steps.\n",
      "T5 causes T4 with a delay of 1 time steps.\n",
      "T2 causes T5 with a delay of 0 time steps.\n",
      "T3 causes T5 with a delay of 6 time steps.\n",
      "T5 causes T5 with a delay of 1 time steps.\n",
      "T4 causes T6 with a delay of 2 time steps.\n",
      "T6 causes T6 with a delay of 1 time steps.\n",
      "T7 causes T6 with a delay of 1 time steps.\n",
      "T7 causes T7 with a delay of 1 time steps.\n",
      "T8 causes T7 with a delay of 1 time steps.\n",
      "T8 causes T8 with a delay of 1 time steps.\n",
      "T9 causes T8 with a delay of 1 time steps.\n",
      "T0 causes T9 with a delay of 1 time steps.\n",
      "T7 causes T9 with a delay of 2 time steps.\n",
      "T9 causes T9 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 28\n",
      "Total False Negatives: 14\n",
      "Total Direct False Positives: 2\n",
      "Total Direct True Positives: 26\n",
      "TPs': ['0->0', '1->0', '8->0', '1->1', '9->1', '0->2', '2->2', '3->2', '1->3', '3->3', '4->3', '1->4', '2->4', '4->4', '5->4', '2->5', '3->5', '5->5', '4->6', '6->6', '7->6', '7->7', '8->7', '8->8', '9->8', '0->9', '7->9', '9->9']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->0', '8->0', '1->1', '9->1', '0->2', '2->2', '3->2', '1->3', '3->3', '4->3', '2->4', '4->4', '5->4', '3->5', '5->5', '4->6', '6->6', '7->6', '7->7', '8->7', '8->8', '9->8', '0->9', '7->9', '9->9']\n",
      "FPs direct: ['1->4', '2->5']\n",
      "FNs: ['9->0', '0->1', '2->1', '1->2', '2->3', '3->4', '4->5', '6->5', '5->6', '5->7', '6->7', '6->8', '7->8', '8->9']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.6666666666666666\n",
      "F1' score: 0.8\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.9285714285714286\n",
      "Recall: 0.65\n",
      "F1 score: 0.7647058823529412\n",
      "Percentage of delays that are correctly discovered: 69.23076923076923%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=512, bias=True)\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=8, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 571307\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/lorenz96/groundtruth.csv\n",
      "===================Results===================\n",
      "T0 causes T0 with a delay of 1 time steps.\n",
      "T8 causes T0 with a delay of 1 time steps.\n",
      "T1 causes T1 with a delay of 4 time steps.\n",
      "T9 causes T1 with a delay of 1 time steps.\n",
      "T0 causes T2 with a delay of 1 time steps.\n",
      "T2 causes T2 with a delay of 1 time steps.\n",
      "T1 causes T3 with a delay of 0 time steps.\n",
      "T3 causes T3 with a delay of 1 time steps.\n",
      "T5 causes T3 with a delay of 0 time steps.\n",
      "T2 causes T4 with a delay of 1 time steps.\n",
      "T4 causes T4 with a delay of 1 time steps.\n",
      "T3 causes T5 with a delay of 1 time steps.\n",
      "T5 causes T5 with a delay of 1 time steps.\n",
      "T4 causes T6 with a delay of 1 time steps.\n",
      "T6 causes T6 with a delay of 1 time steps.\n",
      "T5 causes T7 with a delay of 0 time steps.\n",
      "T7 causes T7 with a delay of 1 time steps.\n",
      "T6 causes T8 with a delay of 1 time steps.\n",
      "T8 causes T8 with a delay of 1 time steps.\n",
      "T7 causes T9 with a delay of 0 time steps.\n",
      "T9 causes T9 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 21\n",
      "Total False Negatives: 20\n",
      "Total Direct False Positives: 1\n",
      "Total Direct True Positives: 20\n",
      "TPs': ['0->0', '8->0', '1->1', '9->1', '0->2', '2->2', '1->3', '3->3', '5->3', '2->4', '4->4', '3->5', '5->5', '4->6', '6->6', '5->7', '7->7', '6->8', '8->8', '7->9', '9->9']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '8->0', '1->1', '9->1', '0->2', '2->2', '1->3', '3->3', '2->4', '4->4', '3->5', '5->5', '4->6', '6->6', '5->7', '7->7', '6->8', '8->8', '7->9', '9->9']\n",
      "FPs direct: ['5->3']\n",
      "FNs: ['1->0', '9->0', '0->1', '2->1', '1->2', '3->2', '2->3', '4->3', '3->4', '5->4', '4->5', '6->5', '5->6', '7->6', '6->7', '8->7', '7->8', '9->8', '0->9', '8->9']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.5121951219512195\n",
      "F1' score: 0.6774193548387097\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.9523809523809523\n",
      "Recall: 0.5\n",
      "F1 score: 0.6557377049180327\n",
      "Percentage of delays that are correctly discovered: 80.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=512, bias=True)\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=8, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 571307\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/lorenz96/groundtruth.csv\n",
      "===================Results===================\n",
      "T0 causes T0 with a delay of 1 time steps.\n",
      "T1 causes T0 with a delay of 1 time steps.\n",
      "T7 causes T0 with a delay of 0 time steps.\n",
      "T8 causes T0 with a delay of 2 time steps.\n",
      "T1 causes T1 with a delay of 1 time steps.\n",
      "T2 causes T1 with a delay of 0 time steps.\n",
      "T3 causes T1 with a delay of 2 time steps.\n",
      "T9 causes T1 with a delay of 10 time steps.\n",
      "T0 causes T2 with a delay of 8 time steps.\n",
      "T2 causes T2 with a delay of 1 time steps.\n",
      "T3 causes T2 with a delay of 1 time steps.\n",
      "T4 causes T2 with a delay of 4 time steps.\n",
      "T9 causes T2 with a delay of 0 time steps.\n",
      "T1 causes T3 with a delay of 2 time steps.\n",
      "T3 causes T3 with a delay of 1 time steps.\n",
      "T4 causes T3 with a delay of 0 time steps.\n",
      "T9 causes T3 with a delay of 0 time steps.\n",
      "T2 causes T4 with a delay of 11 time steps.\n",
      "T4 causes T4 with a delay of 1 time steps.\n",
      "T5 causes T4 with a delay of 1 time steps.\n",
      "T3 causes T5 with a delay of 0 time steps.\n",
      "T5 causes T5 with a delay of 1 time steps.\n",
      "T4 causes T6 with a delay of 2 time steps.\n",
      "T6 causes T6 with a delay of 1 time steps.\n",
      "T7 causes T6 with a delay of 1 time steps.\n",
      "T4 causes T7 with a delay of 2 time steps.\n",
      "T7 causes T7 with a delay of 1 time steps.\n",
      "T7 causes T8 with a delay of 0 time steps.\n",
      "T8 causes T8 with a delay of 1 time steps.\n",
      "T9 causes T8 with a delay of 0 time steps.\n",
      "T0 causes T9 with a delay of 1 time steps.\n",
      "T8 causes T9 with a delay of 0 time steps.\n",
      "T9 causes T9 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 33\n",
      "Total False Negatives: 13\n",
      "Total Direct False Positives: 6\n",
      "Total Direct True Positives: 27\n",
      "TPs': ['0->0', '1->0', '7->0', '8->0', '1->1', '2->1', '3->1', '9->1', '0->2', '2->2', '3->2', '4->2', '9->2', '1->3', '3->3', '4->3', '9->3', '2->4', '4->4', '5->4', '3->5', '5->5', '4->6', '6->6', '7->6', '4->7', '7->7', '7->8', '8->8', '9->8', '0->9', '8->9', '9->9']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->0', '8->0', '1->1', '2->1', '9->1', '0->2', '2->2', '3->2', '1->3', '3->3', '4->3', '2->4', '4->4', '5->4', '3->5', '5->5', '4->6', '6->6', '7->6', '7->7', '7->8', '8->8', '9->8', '0->9', '8->9', '9->9']\n",
      "FPs direct: ['7->0', '3->1', '4->2', '9->2', '9->3', '4->7']\n",
      "FNs: ['9->0', '0->1', '1->2', '2->3', '3->4', '4->5', '6->5', '5->6', '5->7', '6->7', '8->7', '6->8', '7->9']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.717391304347826\n",
      "F1' score: 0.8354430379746834\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.8181818181818182\n",
      "Recall: 0.675\n",
      "F1 score: 0.7397260273972603\n",
      "Percentage of delays that are correctly discovered: 55.55555555555556%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=512, bias=True)\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=8, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 571307\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/lorenz96/groundtruth.csv\n",
      "===================Results===================\n",
      "T0 causes T0 with a delay of 10 time steps.\n",
      "T8 causes T0 with a delay of 0 time steps.\n",
      "T1 causes T1 with a delay of 1 time steps.\n",
      "T9 causes T1 with a delay of 0 time steps.\n",
      "T0 causes T2 with a delay of 0 time steps.\n",
      "T2 causes T2 with a delay of 1 time steps.\n",
      "T3 causes T2 with a delay of 12 time steps.\n",
      "T4 causes T2 with a delay of 0 time steps.\n",
      "T1 causes T3 with a delay of 0 time steps.\n",
      "T3 causes T3 with a delay of 1 time steps.\n",
      "T2 causes T4 with a delay of 1 time steps.\n",
      "T4 causes T4 with a delay of 1 time steps.\n",
      "T3 causes T5 with a delay of 1 time steps.\n",
      "T5 causes T5 with a delay of 6 time steps.\n",
      "T7 causes T5 with a delay of 0 time steps.\n",
      "T4 causes T6 with a delay of 1 time steps.\n",
      "T6 causes T6 with a delay of 3 time steps.\n",
      "T5 causes T7 with a delay of 1 time steps.\n",
      "T7 causes T7 with a delay of 2 time steps.\n",
      "T9 causes T7 with a delay of 0 time steps.\n",
      "T6 causes T8 with a delay of 1 time steps.\n",
      "T8 causes T8 with a delay of 4 time steps.\n",
      "T1 causes T9 with a delay of 0 time steps.\n",
      "T7 causes T9 with a delay of 1 time steps.\n",
      "T9 causes T9 with a delay of 14 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 25\n",
      "Total False Negatives: 19\n",
      "Total Direct False Positives: 4\n",
      "Total Direct True Positives: 21\n",
      "TPs': ['0->0', '8->0', '1->1', '9->1', '0->2', '2->2', '3->2', '4->2', '1->3', '3->3', '2->4', '4->4', '3->5', '5->5', '7->5', '4->6', '6->6', '5->7', '7->7', '9->7', '6->8', '8->8', '1->9', '7->9', '9->9']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '8->0', '1->1', '9->1', '0->2', '2->2', '3->2', '1->3', '3->3', '2->4', '4->4', '3->5', '5->5', '4->6', '6->6', '5->7', '7->7', '6->8', '8->8', '7->9', '9->9']\n",
      "FPs direct: ['4->2', '7->5', '9->7', '1->9']\n",
      "FNs: ['1->0', '9->0', '0->1', '2->1', '1->2', '2->3', '4->3', '3->4', '5->4', '4->5', '6->5', '5->6', '7->6', '6->7', '8->7', '7->8', '9->8', '0->9', '8->9']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.5681818181818182\n",
      "F1' score: 0.7246376811594203\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.84\n",
      "Recall: 0.525\n",
      "F1 score: 0.6461538461538462\n",
      "Percentage of delays that are correctly discovered: 47.61904761904761%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=512, bias=True)\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=8, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 571307\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/lorenz96/groundtruth.csv\n",
      "===================Results===================\n",
      "T0 causes T0 with a delay of 1 time steps.\n",
      "T1 causes T0 with a delay of 1 time steps.\n",
      "T8 causes T0 with a delay of 3 time steps.\n",
      "T1 causes T1 with a delay of 1 time steps.\n",
      "T2 causes T1 with a delay of 1 time steps.\n",
      "T3 causes T1 with a delay of 0 time steps.\n",
      "T9 causes T1 with a delay of 1 time steps.\n",
      "T0 causes T2 with a delay of 0 time steps.\n",
      "T2 causes T2 with a delay of 4 time steps.\n",
      "T9 causes T2 with a delay of 0 time steps.\n",
      "T1 causes T3 with a delay of 0 time steps.\n",
      "T3 causes T3 with a delay of 2 time steps.\n",
      "T2 causes T4 with a delay of 2 time steps.\n",
      "T4 causes T4 with a delay of 1 time steps.\n",
      "T5 causes T4 with a delay of 1 time steps.\n",
      "T3 causes T5 with a delay of 0 time steps.\n",
      "T5 causes T5 with a delay of 2 time steps.\n",
      "T6 causes T5 with a delay of 11 time steps.\n",
      "T7 causes T5 with a delay of 0 time steps.\n",
      "T0 causes T6 with a delay of 9 time steps.\n",
      "T4 causes T6 with a delay of 2 time steps.\n",
      "T6 causes T6 with a delay of 1 time steps.\n",
      "T8 causes T6 with a delay of 7 time steps.\n",
      "T5 causes T7 with a delay of 1 time steps.\n",
      "T7 causes T7 with a delay of 9 time steps.\n",
      "T6 causes T8 with a delay of 0 time steps.\n",
      "T7 causes T8 with a delay of 1 time steps.\n",
      "T8 causes T8 with a delay of 2 time steps.\n",
      "T9 causes T8 with a delay of 4 time steps.\n",
      "T0 causes T9 with a delay of 1 time steps.\n",
      "T1 causes T9 with a delay of 1 time steps.\n",
      "T6 causes T9 with a delay of 0 time steps.\n",
      "T9 causes T9 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 1\n",
      "Total True Positives': 32\n",
      "Total False Negatives: 14\n",
      "Total Direct False Positives: 7\n",
      "Total Direct True Positives: 26\n",
      "TPs': ['0->0', '1->0', '8->0', '1->1', '2->1', '3->1', '9->1', '0->2', '2->2', '9->2', '1->3', '3->3', '2->4', '4->4', '5->4', '3->5', '5->5', '6->5', '7->5', '4->6', '6->6', '8->6', '5->7', '7->7', '6->8', '7->8', '8->8', '9->8', '0->9', '1->9', '6->9', '9->9']\n",
      "FPs': ['0->6']\n",
      "TPs direct: ['0->0', '1->0', '8->0', '1->1', '2->1', '9->1', '0->2', '2->2', '1->3', '3->3', '2->4', '4->4', '5->4', '3->5', '5->5', '6->5', '4->6', '6->6', '5->7', '7->7', '6->8', '7->8', '8->8', '9->8', '0->9', '9->9']\n",
      "FPs direct: ['3->1', '9->2', '7->5', '0->6', '8->6', '1->9', '6->9']\n",
      "FNs: ['9->0', '0->1', '1->2', '3->2', '2->3', '4->3', '3->4', '4->5', '5->6', '7->6', '6->7', '8->7', '7->9', '8->9']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.9696969696969697\n",
      "Recall': 0.6956521739130435\n",
      "F1' score: 0.810126582278481\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.7878787878787878\n",
      "Recall: 0.65\n",
      "F1 score: 0.7123287671232875\n",
      "Percentage of delays that are correctly discovered: 46.15384615384615%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=512, bias=True)\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=8, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 571307\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/lorenz96/groundtruth.csv\n",
      "===================Results===================\n",
      "T0 causes T0 with a delay of 1 time steps.\n",
      "T8 causes T0 with a delay of 0 time steps.\n",
      "T1 causes T1 with a delay of 4 time steps.\n",
      "T9 causes T1 with a delay of 1 time steps.\n",
      "T0 causes T2 with a delay of 0 time steps.\n",
      "T2 causes T2 with a delay of 24 time steps.\n",
      "T1 causes T3 with a delay of 1 time steps.\n",
      "T3 causes T3 with a delay of 1 time steps.\n",
      "T2 causes T4 with a delay of 0 time steps.\n",
      "T4 causes T4 with a delay of 3 time steps.\n",
      "T3 causes T5 with a delay of 1 time steps.\n",
      "T5 causes T5 with a delay of 16 time steps.\n",
      "T4 causes T6 with a delay of 0 time steps.\n",
      "T6 causes T6 with a delay of 4 time steps.\n",
      "T5 causes T7 with a delay of 0 time steps.\n",
      "T7 causes T7 with a delay of 16 time steps.\n",
      "T6 causes T8 with a delay of 1 time steps.\n",
      "T8 causes T8 with a delay of 24 time steps.\n",
      "T7 causes T9 with a delay of 0 time steps.\n",
      "T9 causes T9 with a delay of 2 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 20\n",
      "Total False Negatives: 20\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 20\n",
      "TPs': ['0->0', '8->0', '1->1', '9->1', '0->2', '2->2', '1->3', '3->3', '2->4', '4->4', '3->5', '5->5', '4->6', '6->6', '5->7', '7->7', '6->8', '8->8', '7->9', '9->9']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '8->0', '1->1', '9->1', '0->2', '2->2', '1->3', '3->3', '2->4', '4->4', '3->5', '5->5', '4->6', '6->6', '5->7', '7->7', '6->8', '8->8', '7->9', '9->9']\n",
      "FPs direct: []\n",
      "FNs: ['1->0', '9->0', '0->1', '2->1', '1->2', '3->2', '2->3', '4->3', '3->4', '5->4', '4->5', '6->5', '5->6', '7->6', '6->7', '8->7', '7->8', '9->8', '0->9', '8->9']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.5\n",
      "F1' score: 0.6666666666666666\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.5\n",
      "F1 score: 0.6666666666666666\n",
      "Percentage of delays that are correctly discovered: 30.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=512, bias=True)\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=8, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 571307\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/lorenz96/groundtruth.csv\n",
      "===================Results===================\n",
      "T0 causes T0 with a delay of 1 time steps.\n",
      "T1 causes T0 with a delay of 0 time steps.\n",
      "T8 causes T0 with a delay of 0 time steps.\n",
      "T1 causes T1 with a delay of 6 time steps.\n",
      "T2 causes T1 with a delay of 3 time steps.\n",
      "T3 causes T1 with a delay of 0 time steps.\n",
      "T9 causes T1 with a delay of 1 time steps.\n",
      "T0 causes T2 with a delay of 1 time steps.\n",
      "T3 causes T2 with a delay of 7 time steps.\n",
      "T1 causes T3 with a delay of 6 time steps.\n",
      "T4 causes T3 with a delay of 1 time steps.\n",
      "T2 causes T4 with a delay of 9 time steps.\n",
      "T4 causes T4 with a delay of 1 time steps.\n",
      "T5 causes T4 with a delay of 1 time steps.\n",
      "T3 causes T5 with a delay of 0 time steps.\n",
      "T5 causes T5 with a delay of 4 time steps.\n",
      "T4 causes T6 with a delay of 0 time steps.\n",
      "T6 causes T6 with a delay of 5 time steps.\n",
      "T5 causes T7 with a delay of 0 time steps.\n",
      "T7 causes T7 with a delay of 2 time steps.\n",
      "T6 causes T8 with a delay of 1 time steps.\n",
      "T8 causes T8 with a delay of 12 time steps.\n",
      "T0 causes T9 with a delay of 8 time steps.\n",
      "T7 causes T9 with a delay of 0 time steps.\n",
      "T9 causes T9 with a delay of 3 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 25\n",
      "Total False Negatives: 16\n",
      "Total Direct False Positives: 1\n",
      "Total Direct True Positives: 24\n",
      "TPs': ['0->0', '1->0', '8->0', '1->1', '2->1', '3->1', '9->1', '0->2', '3->2', '1->3', '4->3', '2->4', '4->4', '5->4', '3->5', '5->5', '4->6', '6->6', '5->7', '7->7', '6->8', '8->8', '0->9', '7->9', '9->9']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->0', '8->0', '1->1', '2->1', '9->1', '0->2', '3->2', '1->3', '4->3', '2->4', '4->4', '5->4', '3->5', '5->5', '4->6', '6->6', '5->7', '7->7', '6->8', '8->8', '0->9', '7->9', '9->9']\n",
      "FPs direct: ['3->1']\n",
      "FNs: ['9->0', '0->1', '1->2', '2->2', '2->3', '3->3', '3->4', '4->5', '6->5', '5->6', '7->6', '6->7', '8->7', '7->8', '9->8', '8->9']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.6097560975609756\n",
      "F1' score: 0.7575757575757575\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.96\n",
      "Recall: 0.6\n",
      "F1 score: 0.7384615384615384\n",
      "Percentage of delays that are correctly discovered: 29.166666666666668%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=512, bias=True)\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=8, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 571307\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/lorenz96/groundtruth.csv\n",
      "===================Results===================\n",
      "T0 causes T0 with a delay of 12 time steps.\n",
      "T1 causes T0 with a delay of 3 time steps.\n",
      "T1 causes T1 with a delay of 8 time steps.\n",
      "T2 causes T1 with a delay of 7 time steps.\n",
      "T0 causes T2 with a delay of 0 time steps.\n",
      "T2 causes T2 with a delay of 15 time steps.\n",
      "T1 causes T3 with a delay of 0 time steps.\n",
      "T3 causes T3 with a delay of 5 time steps.\n",
      "T0 causes T4 with a delay of 0 time steps.\n",
      "T2 causes T4 with a delay of 0 time steps.\n",
      "T4 causes T4 with a delay of 4 time steps.\n",
      "T1 causes T5 with a delay of 0 time steps.\n",
      "T3 causes T5 with a delay of 1 time steps.\n",
      "T5 causes T5 with a delay of 5 time steps.\n",
      "T6 causes T5 with a delay of 11 time steps.\n",
      "T4 causes T6 with a delay of 0 time steps.\n",
      "T6 causes T6 with a delay of 12 time steps.\n",
      "T7 causes T6 with a delay of 5 time steps.\n",
      "T5 causes T7 with a delay of 0 time steps.\n",
      "T7 causes T7 with a delay of 3 time steps.\n",
      "T8 causes T7 with a delay of 0 time steps.\n",
      "T9 causes T7 with a delay of 0 time steps.\n",
      "T0 causes T8 with a delay of 0 time steps.\n",
      "T1 causes T8 with a delay of 0 time steps.\n",
      "T6 causes T8 with a delay of 1 time steps.\n",
      "T8 causes T8 with a delay of 3 time steps.\n",
      "T7 causes T9 with a delay of 0 time steps.\n",
      "T9 causes T9 with a delay of 4 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 1\n",
      "Total True Positives': 27\n",
      "Total False Negatives: 17\n",
      "Total Direct False Positives: 5\n",
      "Total Direct True Positives: 23\n",
      "TPs': ['0->0', '1->0', '1->1', '2->1', '0->2', '2->2', '1->3', '3->3', '0->4', '2->4', '4->4', '1->5', '3->5', '5->5', '6->5', '4->6', '6->6', '7->6', '5->7', '7->7', '8->7', '9->7', '0->8', '6->8', '8->8', '7->9', '9->9']\n",
      "FPs': ['1->8']\n",
      "TPs direct: ['0->0', '1->0', '1->1', '2->1', '0->2', '2->2', '1->3', '3->3', '2->4', '4->4', '3->5', '5->5', '6->5', '4->6', '6->6', '7->6', '5->7', '7->7', '8->7', '6->8', '8->8', '7->9', '9->9']\n",
      "FPs direct: ['0->4', '1->5', '9->7', '0->8', '1->8']\n",
      "FNs: ['8->0', '9->0', '0->1', '9->1', '1->2', '3->2', '2->3', '4->3', '3->4', '5->4', '4->5', '5->6', '6->7', '7->8', '9->8', '0->9', '8->9']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.9642857142857143\n",
      "Recall': 0.6136363636363636\n",
      "F1' score: 0.75\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.8214285714285714\n",
      "Recall: 0.575\n",
      "F1 score: 0.676470588235294\n",
      "Percentage of delays that are correctly discovered: 8.695652173913043%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=512, bias=True)\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=8, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 571307\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/lorenz96/groundtruth.csv\n",
      "===================Results===================\n",
      "T0 causes T0 with a delay of 1 time steps.\n",
      "T1 causes T0 with a delay of 1 time steps.\n",
      "T2 causes T0 with a delay of 3 time steps.\n",
      "T8 causes T0 with a delay of 4 time steps.\n",
      "T9 causes T0 with a delay of 0 time steps.\n",
      "T1 causes T1 with a delay of 1 time steps.\n",
      "T2 causes T1 with a delay of 1 time steps.\n",
      "T9 causes T1 with a delay of 9 time steps.\n",
      "T0 causes T2 with a delay of 0 time steps.\n",
      "T2 causes T2 with a delay of 1 time steps.\n",
      "T3 causes T2 with a delay of 1 time steps.\n",
      "T1 causes T3 with a delay of 1 time steps.\n",
      "T2 causes T3 with a delay of 0 time steps.\n",
      "T3 causes T3 with a delay of 15 time steps.\n",
      "T4 causes T3 with a delay of 11 time steps.\n",
      "T2 causes T4 with a delay of 1 time steps.\n",
      "T3 causes T4 with a delay of 1 time steps.\n",
      "T4 causes T4 with a delay of 12 time steps.\n",
      "T5 causes T4 with a delay of 4 time steps.\n",
      "T6 causes T4 with a delay of 0 time steps.\n",
      "T1 causes T5 with a delay of 1 time steps.\n",
      "T5 causes T5 with a delay of 1 time steps.\n",
      "T4 causes T6 with a delay of 1 time steps.\n",
      "T7 causes T6 with a delay of 1 time steps.\n",
      "T0 causes T7 with a delay of 17 time steps.\n",
      "T4 causes T7 with a delay of 3 time steps.\n",
      "T7 causes T7 with a delay of 1 time steps.\n",
      "T0 causes T8 with a delay of 0 time steps.\n",
      "T6 causes T8 with a delay of 1 time steps.\n",
      "T8 causes T8 with a delay of 2 time steps.\n",
      "T6 causes T9 with a delay of 10 time steps.\n",
      "T7 causes T9 with a delay of 0 time steps.\n",
      "T9 causes T9 with a delay of 4 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 1\n",
      "Total True Positives': 32\n",
      "Total False Negatives: 14\n",
      "Total Direct False Positives: 7\n",
      "Total Direct True Positives: 26\n",
      "TPs': ['0->0', '1->0', '2->0', '8->0', '9->0', '1->1', '2->1', '9->1', '0->2', '2->2', '3->2', '1->3', '2->3', '3->3', '4->3', '2->4', '3->4', '4->4', '5->4', '6->4', '1->5', '5->5', '4->6', '7->6', '4->7', '7->7', '0->8', '6->8', '8->8', '6->9', '7->9', '9->9']\n",
      "FPs': ['0->7']\n",
      "TPs direct: ['0->0', '1->0', '8->0', '9->0', '1->1', '2->1', '9->1', '0->2', '2->2', '3->2', '1->3', '2->3', '3->3', '4->3', '2->4', '3->4', '4->4', '5->4', '5->5', '4->6', '7->6', '7->7', '6->8', '8->8', '7->9', '9->9']\n",
      "FPs direct: ['2->0', '6->4', '1->5', '0->7', '4->7', '0->8', '6->9']\n",
      "FNs: ['0->1', '1->2', '3->5', '4->5', '6->5', '5->6', '6->6', '5->7', '6->7', '8->7', '7->8', '9->8', '0->9', '8->9']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.9696969696969697\n",
      "Recall': 0.6956521739130435\n",
      "F1' score: 0.810126582278481\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.7878787878787878\n",
      "Recall: 0.65\n",
      "F1 score: 0.7123287671232875\n",
      "Percentage of delays that are correctly discovered: 53.84615384615385%\n",
      "===================Summary===================\n",
      "\t    Precision'   Recall'       F1'  Precision  Recall        F1       PoD\n",
      "\t1     1.000000  0.545455  0.705882   0.833333   0.500  0.625000  0.700000\n",
      "\t2     1.000000  0.666667  0.800000   0.928571   0.650  0.764706  0.692308\n",
      "\t3     1.000000  0.512195  0.677419   0.952381   0.500  0.655738  0.800000\n",
      "\t4     1.000000  0.717391  0.835443   0.818182   0.675  0.739726  0.555556\n",
      "\t5     1.000000  0.568182  0.724638   0.840000   0.525  0.646154  0.476190\n",
      "\t6     0.969697  0.695652  0.810127   0.787879   0.650  0.712329  0.461538\n",
      "\t7     1.000000  0.500000  0.666667   1.000000   0.500  0.666667  0.300000\n",
      "\t8     1.000000  0.609756  0.757576   0.960000   0.600  0.738462  0.291667\n",
      "\t9     0.964286  0.613636  0.750000   0.821429   0.575  0.676471  0.086957\n",
      "\t10    0.969697  0.695652  0.810127   0.787879   0.650  0.712329  0.538462\n"
     ]
    }
   ],
   "source": [
    "! python runner.py -c config/config_lorenz.json -t lorenz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 145515\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim1_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 1 time steps.\n",
      "2 causes 0 with a delay of 1 time steps.\n",
      "0 causes 1 with a delay of 1 time steps.\n",
      "1 causes 1 with a delay of 1 time steps.\n",
      "2 causes 1 with a delay of 14 time steps.\n",
      "2 causes 2 with a delay of 1 time steps.\n",
      "3 causes 2 with a delay of 1 time steps.\n",
      "3 causes 3 with a delay of 1 time steps.\n",
      "4 causes 3 with a delay of 27 time steps.\n",
      "3 causes 4 with a delay of 1 time steps.\n",
      "4 causes 4 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 2\n",
      "Total True Positives': 9\n",
      "Total False Negatives: 2\n",
      "Total Direct False Positives: 3\n",
      "Total Direct True Positives: 8\n",
      "TPs': ['0->0', '2->0', '1->1', '2->1', '2->2', '3->2', '3->3', '4->3', '4->4']\n",
      "FPs': ['0->1', '3->4']\n",
      "TPs direct: ['0->0', '1->1', '2->1', '2->2', '3->2', '3->3', '4->3', '4->4']\n",
      "FPs direct: ['2->0', '0->1', '3->4']\n",
      "FNs: ['1->0', '4->0']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.8181818181818182\n",
      "Recall': 0.8181818181818182\n",
      "F1' score: 0.8181818181818182\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.7272727272727273\n",
      "Recall: 0.8\n",
      "F1 score: 0.761904761904762\n",
      "Percentage of delays that are correctly discovered: 75.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 155415\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim2_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 12 time steps.\n",
      "2 causes 0 with a delay of 12 time steps.\n",
      "0 causes 1 with a delay of 1 time steps.\n",
      "1 causes 1 with a delay of 1 time steps.\n",
      "2 causes 2 with a delay of 1 time steps.\n",
      "3 causes 2 with a delay of 1 time steps.\n",
      "3 causes 3 with a delay of 1 time steps.\n",
      "4 causes 3 with a delay of 1 time steps.\n",
      "1 causes 4 with a delay of 1 time steps.\n",
      "2 causes 4 with a delay of 1 time steps.\n",
      "4 causes 4 with a delay of 28 time steps.\n",
      "0 causes 5 with a delay of 7 time steps.\n",
      "5 causes 5 with a delay of 1 time steps.\n",
      "5 causes 6 with a delay of 1 time steps.\n",
      "6 causes 6 with a delay of 1 time steps.\n",
      "2 causes 7 with a delay of 1 time steps.\n",
      "3 causes 7 with a delay of 1 time steps.\n",
      "7 causes 7 with a delay of 1 time steps.\n",
      "8 causes 8 with a delay of 1 time steps.\n",
      "9 causes 8 with a delay of 1 time steps.\n",
      "5 causes 9 with a delay of 1 time steps.\n",
      "9 causes 9 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 8\n",
      "Total True Positives': 14\n",
      "Total False Negatives: 8\n",
      "Total Direct False Positives: 9\n",
      "Total Direct True Positives: 13\n",
      "TPs': ['0->0', '2->0', '1->1', '2->2', '3->2', '3->3', '4->3', '4->4', '5->5', '6->6', '7->7', '8->8', '9->8', '9->9']\n",
      "FPs': ['0->1', '1->4', '2->4', '0->5', '5->6', '2->7', '3->7', '5->9']\n",
      "TPs direct: ['0->0', '1->1', '2->2', '3->2', '3->3', '4->3', '4->4', '5->5', '6->6', '7->7', '8->8', '9->8', '9->9']\n",
      "FPs direct: ['2->0', '0->1', '1->4', '2->4', '0->5', '5->6', '2->7', '3->7', '5->9']\n",
      "FNs: ['1->0', '4->0', '2->1', '7->2', '6->5', '9->5', '7->6', '8->7']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.6363636363636364\n",
      "Recall': 0.6363636363636364\n",
      "F1' score: 0.6363636363636364\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.5909090909090909\n",
      "Recall: 0.6190476190476191\n",
      "F1 score: 0.6046511627906977\n",
      "Percentage of delays that are correctly discovered: 84.61538461538461%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 171915\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim3_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 20 time steps.\n",
      "1 causes 1 with a delay of 6 time steps.\n",
      "2 causes 2 with a delay of 31 time steps.\n",
      "3 causes 2 with a delay of 31 time steps.\n",
      "3 causes 3 with a delay of 28 time steps.\n",
      "4 causes 4 with a delay of 5 time steps.\n",
      "5 causes 5 with a delay of 11 time steps.\n",
      "0 causes 6 with a delay of 13 time steps.\n",
      "5 causes 6 with a delay of 4 time steps.\n",
      "6 causes 6 with a delay of 16 time steps.\n",
      "7 causes 6 with a delay of 16 time steps.\n",
      "9 causes 6 with a delay of 14 time steps.\n",
      "7 causes 7 with a delay of 1 time steps.\n",
      "8 causes 8 with a delay of 10 time steps.\n",
      "3 causes 9 with a delay of 6 time steps.\n",
      "7 causes 9 with a delay of 5 time steps.\n",
      "8 causes 9 with a delay of 24 time steps.\n",
      "9 causes 9 with a delay of 6 time steps.\n",
      "10 causes 10 with a delay of 2 time steps.\n",
      "11 causes 10 with a delay of 2 time steps.\n",
      "11 causes 11 with a delay of 2 time steps.\n",
      "2 causes 12 with a delay of 28 time steps.\n",
      "3 causes 12 with a delay of 1 time steps.\n",
      "11 causes 12 with a delay of 1 time steps.\n",
      "12 causes 12 with a delay of 11 time steps.\n",
      "13 causes 12 with a delay of 5 time steps.\n",
      "14 causes 12 with a delay of 2 time steps.\n",
      "13 causes 13 with a delay of 3 time steps.\n",
      "3 causes 14 with a delay of 7 time steps.\n",
      "14 causes 14 with a delay of 2 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 10\n",
      "Total True Positives': 20\n",
      "Total False Negatives: 14\n",
      "Total Direct False Positives: 11\n",
      "Total Direct True Positives: 19\n",
      "TPs': ['0->0', '1->1', '2->2', '3->2', '3->3', '4->4', '5->5', '6->6', '7->6', '7->7', '8->8', '9->9', '10->10', '11->10', '11->11', '12->12', '13->12', '14->12', '13->13', '14->14']\n",
      "FPs': ['0->6', '5->6', '9->6', '3->9', '7->9', '8->9', '2->12', '3->12', '11->12', '3->14']\n",
      "TPs direct: ['0->0', '1->1', '2->2', '3->2', '3->3', '4->4', '5->5', '6->6', '7->6', '7->7', '8->8', '9->9', '10->10', '11->10', '11->11', '12->12', '13->12', '13->13', '14->14']\n",
      "FPs direct: ['0->6', '5->6', '9->6', '3->9', '7->9', '8->9', '2->12', '3->12', '11->12', '14->12', '3->14']\n",
      "FNs: ['1->0', '4->0', '2->1', '7->2', '12->2', '4->3', '6->5', '9->5', '8->7', '12->7', '9->8', '14->10', '12->11', '14->13']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.6666666666666666\n",
      "Recall': 0.5882352941176471\n",
      "F1' score: 0.625\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.6333333333333333\n",
      "Recall: 0.5757575757575758\n",
      "F1 score: 0.6031746031746033\n",
      "Percentage of delays that are correctly discovered: 5.263157894736842%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 472215\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim4_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 10 time steps.\n",
      "1 causes 1 with a delay of 28 time steps.\n",
      "2 causes 2 with a delay of 27 time steps.\n",
      "3 causes 3 with a delay of 5 time steps.\n",
      "4 causes 4 with a delay of 31 time steps.\n",
      "5 causes 5 with a delay of 1 time steps.\n",
      "6 causes 6 with a delay of 17 time steps.\n",
      "7 causes 7 with a delay of 3 time steps.\n",
      "8 causes 8 with a delay of 1 time steps.\n",
      "9 causes 9 with a delay of 1 time steps.\n",
      "10 causes 10 with a delay of 5 time steps.\n",
      "11 causes 11 with a delay of 12 time steps.\n",
      "12 causes 12 with a delay of 22 time steps.\n",
      "13 causes 13 with a delay of 1 time steps.\n",
      "14 causes 14 with a delay of 26 time steps.\n",
      "15 causes 15 with a delay of 30 time steps.\n",
      "16 causes 16 with a delay of 28 time steps.\n",
      "17 causes 17 with a delay of 3 time steps.\n",
      "18 causes 18 with a delay of 7 time steps.\n",
      "19 causes 19 with a delay of 21 time steps.\n",
      "20 causes 20 with a delay of 1 time steps.\n",
      "21 causes 21 with a delay of 1 time steps.\n",
      "22 causes 22 with a delay of 25 time steps.\n",
      "23 causes 23 with a delay of 1 time steps.\n",
      "24 causes 24 with a delay of 27 time steps.\n",
      "25 causes 25 with a delay of 4 time steps.\n",
      "26 causes 26 with a delay of 11 time steps.\n",
      "27 causes 27 with a delay of 6 time steps.\n",
      "28 causes 28 with a delay of 6 time steps.\n",
      "29 causes 29 with a delay of 11 time steps.\n",
      "30 causes 30 with a delay of 16 time steps.\n",
      "31 causes 31 with a delay of 1 time steps.\n",
      "32 causes 32 with a delay of 30 time steps.\n",
      "33 causes 33 with a delay of 6 time steps.\n",
      "34 causes 34 with a delay of 1 time steps.\n",
      "35 causes 35 with a delay of 1 time steps.\n",
      "36 causes 36 with a delay of 1 time steps.\n",
      "37 causes 37 with a delay of 4 time steps.\n",
      "38 causes 38 with a delay of 1 time steps.\n",
      "39 causes 39 with a delay of 3 time steps.\n",
      "40 causes 40 with a delay of 1 time steps.\n",
      "41 causes 41 with a delay of 4 time steps.\n",
      "42 causes 42 with a delay of 1 time steps.\n",
      "43 causes 43 with a delay of 8 time steps.\n",
      "44 causes 44 with a delay of 13 time steps.\n",
      "45 causes 45 with a delay of 1 time steps.\n",
      "46 causes 46 with a delay of 1 time steps.\n",
      "47 causes 47 with a delay of 28 time steps.\n",
      "48 causes 48 with a delay of 1 time steps.\n",
      "49 causes 49 with a delay of 4 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 50\n",
      "Total False Negatives: 61\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 50\n",
      "TPs': ['0->0', '1->1', '2->2', '3->3', '4->4', '5->5', '6->6', '7->7', '8->8', '9->9', '10->10', '11->11', '12->12', '13->13', '14->14', '15->15', '16->16', '17->17', '18->18', '19->19', '20->20', '21->21', '22->22', '23->23', '24->24', '25->25', '26->26', '27->27', '28->28', '29->29', '30->30', '31->31', '32->32', '33->33', '34->34', '35->35', '36->36', '37->37', '38->38', '39->39', '40->40', '41->41', '42->42', '43->43', '44->44', '45->45', '46->46', '47->47', '48->48', '49->49']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->2', '3->3', '4->4', '5->5', '6->6', '7->7', '8->8', '9->9', '10->10', '11->11', '12->12', '13->13', '14->14', '15->15', '16->16', '17->17', '18->18', '19->19', '20->20', '21->21', '22->22', '23->23', '24->24', '25->25', '26->26', '27->27', '28->28', '29->29', '30->30', '31->31', '32->32', '33->33', '34->34', '35->35', '36->36', '37->37', '38->38', '39->39', '40->40', '41->41', '42->42', '43->43', '44->44', '45->45', '46->46', '47->47', '48->48', '49->49']\n",
      "FPs direct: []\n",
      "FNs: ['1->0', '4->0', '2->1', '3->2', '7->2', '22->2', '27->2', '4->3', '6->5', '9->5', '7->6', '8->7', '12->7', '9->8', '11->10', '14->10', '12->11', '13->12', '17->12', '14->13', '16->15', '19->15', '17->16', '18->17', '22->17', '19->18', '21->20', '24->20', '22->21', '23->22', '24->23', '26->25', '29->25', '27->26', '28->27', '32->27', '47->27', '29->28', '31->30', '34->30', '32->31', '33->32', '37->32', '34->33', '36->35', '39->35', '37->36', '38->37', '42->37', '39->38', '41->40', '44->40', '42->41', '43->42', '47->42', '44->43', '46->45', '49->45', '47->46', '48->47', '49->48']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.45045045045045046\n",
      "F1' score: 0.6211180124223602\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.45045045045045046\n",
      "F1 score: 0.6211180124223602\n",
      "Percentage of delays that are correctly discovered: 34.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 145515\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim5_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 1 time steps.\n",
      "3 causes 0 with a delay of 4 time steps.\n",
      "1 causes 1 with a delay of 1 time steps.\n",
      "4 causes 1 with a delay of 10 time steps.\n",
      "4 causes 2 with a delay of 10 time steps.\n",
      "3 causes 3 with a delay of 1 time steps.\n",
      "2 causes 4 with a delay of 8 time steps.\n",
      "4 causes 4 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 3\n",
      "Total True Positives': 5\n",
      "Total False Negatives: 6\n",
      "Total Direct False Positives: 4\n",
      "Total Direct True Positives: 4\n",
      "TPs': ['0->0', '1->1', '4->2', '3->3', '4->4']\n",
      "FPs': ['3->0', '4->1', '2->4']\n",
      "TPs direct: ['0->0', '1->1', '3->3', '4->4']\n",
      "FPs direct: ['3->0', '4->1', '4->2', '2->4']\n",
      "FNs: ['1->0', '4->0', '2->1', '2->2', '3->2', '4->3']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.625\n",
      "Recall': 0.45454545454545453\n",
      "F1' score: 0.5263157894736842\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.5\n",
      "Recall: 0.4\n",
      "F1 score: 0.4444444444444445\n",
      "Percentage of delays that are correctly discovered: 100.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 155415\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim6_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 1 time steps.\n",
      "1 causes 0 with a delay of 13 time steps.\n",
      "0 causes 1 with a delay of 13 time steps.\n",
      "1 causes 1 with a delay of 1 time steps.\n",
      "2 causes 2 with a delay of 1 time steps.\n",
      "3 causes 3 with a delay of 1 time steps.\n",
      "4 causes 4 with a delay of 1 time steps.\n",
      "5 causes 5 with a delay of 1 time steps.\n",
      "6 causes 6 with a delay of 1 time steps.\n",
      "7 causes 7 with a delay of 1 time steps.\n",
      "8 causes 7 with a delay of 14 time steps.\n",
      "8 causes 8 with a delay of 1 time steps.\n",
      "9 causes 9 with a delay of 26 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 1\n",
      "Total True Positives': 12\n",
      "Total False Negatives: 9\n",
      "Total Direct False Positives: 1\n",
      "Total Direct True Positives: 12\n",
      "TPs': ['0->0', '1->0', '1->1', '2->2', '3->3', '4->4', '5->5', '6->6', '7->7', '8->7', '8->8', '9->9']\n",
      "FPs': ['0->1']\n",
      "TPs direct: ['0->0', '1->0', '1->1', '2->2', '3->3', '4->4', '5->5', '6->6', '7->7', '8->7', '8->8', '9->9']\n",
      "FPs direct: ['0->1']\n",
      "FNs: ['4->0', '2->1', '3->2', '7->2', '4->3', '6->5', '9->5', '7->6', '9->8']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.9230769230769231\n",
      "Recall': 0.5714285714285714\n",
      "F1' score: 0.7058823529411765\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.9230769230769231\n",
      "Recall: 0.5714285714285714\n",
      "F1 score: 0.7058823529411765\n",
      "Percentage of delays that are correctly discovered: 75.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 145515\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim7_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 2 time steps.\n",
      "1 causes 1 with a delay of 7 time steps.\n",
      "2 causes 2 with a delay of 2 time steps.\n",
      "3 causes 3 with a delay of 2 time steps.\n",
      "4 causes 4 with a delay of 2 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 5\n",
      "Total False Negatives: 5\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 5\n",
      "TPs': ['0->0', '1->1', '2->2', '3->3', '4->4']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->2', '3->3', '4->4']\n",
      "FPs direct: []\n",
      "FNs: ['1->0', '4->0', '2->1', '3->2', '4->3']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.5\n",
      "F1' score: 0.6666666666666666\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.5\n",
      "F1 score: 0.6666666666666666\n",
      "Percentage of delays that are correctly discovered: 0.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 145515\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim8_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 1 time steps.\n",
      "4 causes 0 with a delay of 18 time steps.\n",
      "1 causes 1 with a delay of 1 time steps.\n",
      "2 causes 1 with a delay of 1 time steps.\n",
      "4 causes 1 with a delay of 1 time steps.\n",
      "2 causes 2 with a delay of 1 time steps.\n",
      "3 causes 2 with a delay of 1 time steps.\n",
      "4 causes 2 with a delay of 1 time steps.\n",
      "3 causes 3 with a delay of 1 time steps.\n",
      "4 causes 3 with a delay of 28 time steps.\n",
      "3 causes 4 with a delay of 1 time steps.\n",
      "4 causes 4 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 2\n",
      "Total True Positives': 10\n",
      "Total False Negatives: 1\n",
      "Total Direct False Positives: 3\n",
      "Total Direct True Positives: 9\n",
      "TPs': ['0->0', '4->0', '1->1', '2->1', '2->2', '3->2', '4->2', '3->3', '4->3', '4->4']\n",
      "FPs': ['4->1', '3->4']\n",
      "TPs direct: ['0->0', '4->0', '1->1', '2->1', '2->2', '3->2', '3->3', '4->3', '4->4']\n",
      "FPs direct: ['4->1', '4->2', '3->4']\n",
      "FNs: ['1->0']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.8333333333333334\n",
      "Recall': 0.9090909090909091\n",
      "F1' score: 0.8695652173913043\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.75\n",
      "Recall: 0.9\n",
      "F1 score: 0.8181818181818182\n",
      "Percentage of delays that are correctly discovered: 77.77777777777779%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 145515\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim9_gt_processed.csv\n",
      "===================Results===================\n",
      "4 causes 0 with a delay of 30 time steps.\n",
      "4 causes 1 with a delay of 29 time steps.\n",
      "4 causes 2 with a delay of 29 time steps.\n",
      "4 causes 3 with a delay of 28 time steps.\n",
      "3 causes 4 with a delay of 20 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 2\n",
      "Total True Positives': 3\n",
      "Total False Negatives: 8\n",
      "Total Direct False Positives: 3\n",
      "Total Direct True Positives: 2\n",
      "TPs': ['4->0', '4->2', '4->3']\n",
      "FPs': ['4->1', '3->4']\n",
      "TPs direct: ['4->0', '4->3']\n",
      "FPs direct: ['4->1', '4->2', '3->4']\n",
      "FNs: ['0->0', '1->0', '1->1', '2->1', '2->2', '3->2', '3->3', '4->4']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.6\n",
      "Recall': 0.2727272727272727\n",
      "F1' score: 0.37499999999999994\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.4\n",
      "Recall: 0.2\n",
      "F1 score: 0.26666666666666666\n",
      "Percentage of delays that are correctly discovered: 0.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 145515\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim10_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 6 time steps.\n",
      "2 causes 0 with a delay of 4 time steps.\n",
      "1 causes 1 with a delay of 10 time steps.\n",
      "2 causes 1 with a delay of 4 time steps.\n",
      "2 causes 2 with a delay of 11 time steps.\n",
      "3 causes 2 with a delay of 10 time steps.\n",
      "3 causes 3 with a delay of 31 time steps.\n",
      "4 causes 4 with a delay of 10 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 8\n",
      "Total False Negatives: 3\n",
      "Total Direct False Positives: 1\n",
      "Total Direct True Positives: 7\n",
      "TPs': ['0->0', '2->0', '1->1', '2->1', '2->2', '3->2', '3->3', '4->4']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->1', '2->2', '3->2', '3->3', '4->4']\n",
      "FPs direct: ['2->0']\n",
      "FNs: ['1->0', '4->0', '4->3']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.7272727272727273\n",
      "F1' score: 0.8421052631578948\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.875\n",
      "Recall: 0.7\n",
      "F1 score: 0.7777777777777777\n",
      "Percentage of delays that are correctly discovered: 0.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 155415\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim11_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 1 time steps.\n",
      "2 causes 0 with a delay of 1 time steps.\n",
      "0 causes 1 with a delay of 1 time steps.\n",
      "1 causes 1 with a delay of 1 time steps.\n",
      "2 causes 2 with a delay of 1 time steps.\n",
      "3 causes 2 with a delay of 1 time steps.\n",
      "3 causes 3 with a delay of 1 time steps.\n",
      "2 causes 4 with a delay of 1 time steps.\n",
      "4 causes 4 with a delay of 1 time steps.\n",
      "0 causes 5 with a delay of 8 time steps.\n",
      "5 causes 5 with a delay of 1 time steps.\n",
      "6 causes 6 with a delay of 1 time steps.\n",
      "0 causes 7 with a delay of 1 time steps.\n",
      "7 causes 7 with a delay of 1 time steps.\n",
      "8 causes 8 with a delay of 1 time steps.\n",
      "9 causes 9 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 4\n",
      "Total True Positives': 12\n",
      "Total False Negatives: 10\n",
      "Total Direct False Positives: 5\n",
      "Total Direct True Positives: 11\n",
      "TPs': ['0->0', '2->0', '1->1', '2->2', '3->2', '3->3', '4->4', '5->5', '6->6', '7->7', '8->8', '9->9']\n",
      "FPs': ['0->1', '2->4', '0->5', '0->7']\n",
      "TPs direct: ['0->0', '1->1', '2->2', '3->2', '3->3', '4->4', '5->5', '6->6', '7->7', '8->8', '9->9']\n",
      "FPs direct: ['2->0', '0->1', '2->4', '0->5', '0->7']\n",
      "FNs: ['1->0', '4->0', '2->1', '7->2', '4->3', '6->5', '9->5', '7->6', '8->7', '9->8']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.75\n",
      "Recall': 0.5454545454545454\n",
      "F1' score: 0.631578947368421\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.6875\n",
      "Recall: 0.5238095238095238\n",
      "F1 score: 0.5945945945945946\n",
      "Percentage of delays that are correctly discovered: 100.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 155415\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim12_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 1 time steps.\n",
      "2 causes 0 with a delay of 26 time steps.\n",
      "0 causes 1 with a delay of 1 time steps.\n",
      "1 causes 1 with a delay of 1 time steps.\n",
      "2 causes 1 with a delay of 1 time steps.\n",
      "1 causes 2 with a delay of 10 time steps.\n",
      "2 causes 2 with a delay of 1 time steps.\n",
      "3 causes 3 with a delay of 1 time steps.\n",
      "2 causes 4 with a delay of 1 time steps.\n",
      "4 causes 4 with a delay of 1 time steps.\n",
      "0 causes 5 with a delay of 19 time steps.\n",
      "5 causes 5 with a delay of 1 time steps.\n",
      "5 causes 6 with a delay of 1 time steps.\n",
      "6 causes 6 with a delay of 1 time steps.\n",
      "2 causes 7 with a delay of 1 time steps.\n",
      "7 causes 7 with a delay of 1 time steps.\n",
      "8 causes 8 with a delay of 1 time steps.\n",
      "9 causes 8 with a delay of 1 time steps.\n",
      "5 causes 9 with a delay of 1 time steps.\n",
      "9 causes 9 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 7\n",
      "Total True Positives': 13\n",
      "Total False Negatives: 9\n",
      "Total Direct False Positives: 8\n",
      "Total Direct True Positives: 12\n",
      "TPs': ['0->0', '2->0', '1->1', '2->1', '2->2', '3->3', '4->4', '5->5', '6->6', '7->7', '8->8', '9->8', '9->9']\n",
      "FPs': ['0->1', '1->2', '2->4', '0->5', '5->6', '2->7', '5->9']\n",
      "TPs direct: ['0->0', '1->1', '2->1', '2->2', '3->3', '4->4', '5->5', '6->6', '7->7', '8->8', '9->8', '9->9']\n",
      "FPs direct: ['2->0', '0->1', '1->2', '2->4', '0->5', '5->6', '2->7', '5->9']\n",
      "FNs: ['1->0', '4->0', '3->2', '7->2', '4->3', '6->5', '9->5', '7->6', '8->7']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.65\n",
      "Recall': 0.5909090909090909\n",
      "F1' score: 0.6190476190476191\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.6\n",
      "Recall: 0.5714285714285714\n",
      "F1 score: 0.5853658536585366\n",
      "Percentage of delays that are correctly discovered: 100.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 145515\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim13_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 30 time steps.\n",
      "1 causes 1 with a delay of 30 time steps.\n",
      "2 causes 2 with a delay of 30 time steps.\n",
      "3 causes 3 with a delay of 7 time steps.\n",
      "4 causes 4 with a delay of 6 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 5\n",
      "Total False Negatives: 8\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 5\n",
      "TPs': ['0->0', '1->1', '2->2', '3->3', '4->4']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->2', '3->3', '4->4']\n",
      "FPs direct: []\n",
      "FNs: ['1->0', '4->0', '0->1', '2->1', '1->2', '3->2', '4->3', '3->4']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.38461538461538464\n",
      "F1' score: 0.5555555555555556\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.38461538461538464\n",
      "F1 score: 0.5555555555555556\n",
      "Percentage of delays that are correctly discovered: 0.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 145515\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim14_gt_processed.csv\n",
      "===================Results===================\n",
      "2 causes 0 with a delay of 16 time steps.\n",
      "1 causes 1 with a delay of 1 time steps.\n",
      "2 causes 1 with a delay of 17 time steps.\n",
      "1 causes 2 with a delay of 1 time steps.\n",
      "3 causes 2 with a delay of 13 time steps.\n",
      "0 causes 3 with a delay of 8 time steps.\n",
      "1 causes 3 with a delay of 19 time steps.\n",
      "3 causes 3 with a delay of 1 time steps.\n",
      "1 causes 4 with a delay of 28 time steps.\n",
      "3 causes 4 with a delay of 19 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 3\n",
      "Total True Positives': 7\n",
      "Total False Negatives: 6\n",
      "Total Direct False Positives: 6\n",
      "Total Direct True Positives: 4\n",
      "TPs': ['2->0', '1->1', '2->1', '3->2', '0->3', '3->3', '1->4']\n",
      "FPs': ['1->2', '1->3', '3->4']\n",
      "TPs direct: ['1->1', '2->1', '3->2', '3->3']\n",
      "FPs direct: ['2->0', '1->2', '0->3', '1->3', '1->4', '3->4']\n",
      "FNs: ['0->0', '1->0', '2->2', '4->3', '0->4', '4->4']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.7\n",
      "Recall': 0.5384615384615384\n",
      "F1' score: 0.608695652173913\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.4\n",
      "Recall: 0.4\n",
      "F1 score: 0.4000000000000001\n",
      "Percentage of delays that are correctly discovered: 50.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 145515\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim15_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 1 time steps.\n",
      "2 causes 0 with a delay of 1 time steps.\n",
      "4 causes 0 with a delay of 1 time steps.\n",
      "1 causes 1 with a delay of 1 time steps.\n",
      "2 causes 1 with a delay of 15 time steps.\n",
      "2 causes 2 with a delay of 1 time steps.\n",
      "3 causes 2 with a delay of 1 time steps.\n",
      "4 causes 2 with a delay of 1 time steps.\n",
      "3 causes 3 with a delay of 1 time steps.\n",
      "4 causes 3 with a delay of 1 time steps.\n",
      "4 causes 4 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 11\n",
      "Total False Negatives: 1\n",
      "Total Direct False Positives: 2\n",
      "Total Direct True Positives: 9\n",
      "TPs': ['0->0', '2->0', '4->0', '1->1', '2->1', '2->2', '3->2', '4->2', '3->3', '4->3', '4->4']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '4->0', '1->1', '2->1', '2->2', '3->2', '3->3', '4->3', '4->4']\n",
      "FPs direct: ['2->0', '4->2']\n",
      "FNs: ['1->0']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.9166666666666666\n",
      "F1' score: 0.9565217391304348\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.8181818181818182\n",
      "Recall: 0.9\n",
      "F1 score: 0.8571428571428572\n",
      "Percentage of delays that are correctly discovered: 88.88888888888889%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 145515\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim16_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 17 time steps.\n",
      "4 causes 0 with a delay of 17 time steps.\n",
      "0 causes 1 with a delay of 1 time steps.\n",
      "1 causes 1 with a delay of 13 time steps.\n",
      "2 causes 2 with a delay of 12 time steps.\n",
      "3 causes 2 with a delay of 11 time steps.\n",
      "0 causes 3 with a delay of 10 time steps.\n",
      "3 causes 3 with a delay of 9 time steps.\n",
      "0 causes 4 with a delay of 23 time steps.\n",
      "3 causes 4 with a delay of 18 time steps.\n",
      "4 causes 4 with a delay of 18 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 4\n",
      "Total True Positives': 7\n",
      "Total False Negatives: 5\n",
      "Total Direct False Positives: 4\n",
      "Total Direct True Positives: 7\n",
      "TPs': ['0->0', '4->0', '1->1', '2->2', '3->2', '3->3', '4->4']\n",
      "FPs': ['0->1', '0->3', '0->4', '3->4']\n",
      "TPs direct: ['0->0', '4->0', '1->1', '2->2', '3->2', '3->3', '4->4']\n",
      "FPs direct: ['0->1', '0->3', '0->4', '3->4']\n",
      "FNs: ['1->0', '2->1', '3->1', '4->2', '4->3']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.6363636363636364\n",
      "Recall': 0.5833333333333334\n",
      "F1' score: 0.6086956521739131\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.6363636363636364\n",
      "Recall: 0.5833333333333334\n",
      "F1 score: 0.6086956521739131\n",
      "Percentage of delays that are correctly discovered: 0.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 155415\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim17_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 1 time steps.\n",
      "2 causes 0 with a delay of 1 time steps.\n",
      "0 causes 1 with a delay of 6 time steps.\n",
      "1 causes 1 with a delay of 1 time steps.\n",
      "2 causes 1 with a delay of 5 time steps.\n",
      "2 causes 2 with a delay of 1 time steps.\n",
      "3 causes 2 with a delay of 1 time steps.\n",
      "3 causes 3 with a delay of 6 time steps.\n",
      "4 causes 3 with a delay of 6 time steps.\n",
      "0 causes 4 with a delay of 1 time steps.\n",
      "2 causes 4 with a delay of 1 time steps.\n",
      "3 causes 4 with a delay of 1 time steps.\n",
      "4 causes 4 with a delay of 5 time steps.\n",
      "0 causes 5 with a delay of 6 time steps.\n",
      "5 causes 5 with a delay of 6 time steps.\n",
      "5 causes 6 with a delay of 1 time steps.\n",
      "6 causes 6 with a delay of 1 time steps.\n",
      "7 causes 6 with a delay of 1 time steps.\n",
      "3 causes 7 with a delay of 1 time steps.\n",
      "5 causes 7 with a delay of 1 time steps.\n",
      "7 causes 7 with a delay of 1 time steps.\n",
      "8 causes 8 with a delay of 1 time steps.\n",
      "9 causes 8 with a delay of 1 time steps.\n",
      "3 causes 9 with a delay of 7 time steps.\n",
      "5 causes 9 with a delay of 1 time steps.\n",
      "9 causes 9 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 10\n",
      "Total True Positives': 16\n",
      "Total False Negatives: 6\n",
      "Total Direct False Positives: 11\n",
      "Total Direct True Positives: 15\n",
      "TPs': ['0->0', '2->0', '1->1', '2->1', '2->2', '3->2', '3->3', '4->3', '4->4', '5->5', '6->6', '7->6', '7->7', '8->8', '9->8', '9->9']\n",
      "FPs': ['0->1', '0->4', '2->4', '3->4', '0->5', '5->6', '3->7', '5->7', '3->9', '5->9']\n",
      "TPs direct: ['0->0', '1->1', '2->1', '2->2', '3->2', '3->3', '4->3', '4->4', '5->5', '6->6', '7->6', '7->7', '8->8', '9->8', '9->9']\n",
      "FPs direct: ['2->0', '0->1', '0->4', '2->4', '3->4', '0->5', '5->6', '3->7', '5->7', '3->9', '5->9']\n",
      "FNs: ['1->0', '4->0', '7->2', '6->5', '9->5', '8->7']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.6153846153846154\n",
      "Recall': 0.7272727272727273\n",
      "F1' score: 0.6666666666666667\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.5769230769230769\n",
      "Recall: 0.7142857142857143\n",
      "F1 score: 0.6382978723404256\n",
      "Percentage of delays that are correctly discovered: 66.66666666666666%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 145515\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim18_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 1 time steps.\n",
      "2 causes 0 with a delay of 1 time steps.\n",
      "0 causes 1 with a delay of 11 time steps.\n",
      "1 causes 1 with a delay of 1 time steps.\n",
      "1 causes 2 with a delay of 1 time steps.\n",
      "2 causes 2 with a delay of 1 time steps.\n",
      "3 causes 2 with a delay of 11 time steps.\n",
      "3 causes 3 with a delay of 1 time steps.\n",
      "4 causes 3 with a delay of 25 time steps.\n",
      "3 causes 4 with a delay of 1 time steps.\n",
      "4 causes 4 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 3\n",
      "Total True Positives': 8\n",
      "Total False Negatives: 3\n",
      "Total Direct False Positives: 4\n",
      "Total Direct True Positives: 7\n",
      "TPs': ['0->0', '2->0', '1->1', '2->2', '3->2', '3->3', '4->3', '4->4']\n",
      "FPs': ['0->1', '1->2', '3->4']\n",
      "TPs direct: ['0->0', '1->1', '2->2', '3->2', '3->3', '4->3', '4->4']\n",
      "FPs direct: ['2->0', '0->1', '1->2', '3->4']\n",
      "FNs: ['1->0', '4->0', '2->1']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.7272727272727273\n",
      "Recall': 0.7272727272727273\n",
      "F1' score: 0.7272727272727273\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.6363636363636364\n",
      "Recall: 0.7\n",
      "F1 score: 0.6666666666666666\n",
      "Percentage of delays that are correctly discovered: 71.42857142857143%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 145515\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim19_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 5 time steps.\n",
      "1 causes 1 with a delay of 5 time steps.\n",
      "2 causes 2 with a delay of 5 time steps.\n",
      "3 causes 3 with a delay of 5 time steps.\n",
      "4 causes 4 with a delay of 5 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 5\n",
      "Total False Negatives: 5\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 5\n",
      "TPs': ['0->0', '1->1', '2->2', '3->3', '4->4']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->2', '3->3', '4->4']\n",
      "FPs direct: []\n",
      "FNs: ['1->0', '4->0', '2->1', '3->2', '4->3']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.5\n",
      "F1' score: 0.6666666666666666\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.5\n",
      "F1 score: 0.6666666666666666\n",
      "Percentage of delays that are correctly discovered: 0.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 145515\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim20_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 5 time steps.\n",
      "1 causes 1 with a delay of 5 time steps.\n",
      "2 causes 2 with a delay of 5 time steps.\n",
      "3 causes 3 with a delay of 5 time steps.\n",
      "4 causes 4 with a delay of 5 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 5\n",
      "Total False Negatives: 5\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 5\n",
      "TPs': ['0->0', '1->1', '2->2', '3->3', '4->4']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->2', '3->3', '4->4']\n",
      "FPs direct: []\n",
      "FNs: ['1->0', '4->0', '2->1', '3->2', '4->3']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.5\n",
      "F1' score: 0.6666666666666666\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.5\n",
      "F1 score: 0.6666666666666666\n",
      "Percentage of delays that are correctly discovered: 0.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 145515\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim21_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 1 time steps.\n",
      "2 causes 0 with a delay of 1 time steps.\n",
      "0 causes 1 with a delay of 1 time steps.\n",
      "1 causes 1 with a delay of 1 time steps.\n",
      "2 causes 1 with a delay of 14 time steps.\n",
      "2 causes 2 with a delay of 1 time steps.\n",
      "3 causes 2 with a delay of 1 time steps.\n",
      "3 causes 3 with a delay of 1 time steps.\n",
      "4 causes 3 with a delay of 27 time steps.\n",
      "3 causes 4 with a delay of 1 time steps.\n",
      "4 causes 4 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 2\n",
      "Total True Positives': 9\n",
      "Total False Negatives: 2\n",
      "Total Direct False Positives: 3\n",
      "Total Direct True Positives: 8\n",
      "TPs': ['0->0', '2->0', '1->1', '2->1', '2->2', '3->2', '3->3', '4->3', '4->4']\n",
      "FPs': ['0->1', '3->4']\n",
      "TPs direct: ['0->0', '1->1', '2->1', '2->2', '3->2', '3->3', '4->3', '4->4']\n",
      "FPs direct: ['2->0', '0->1', '3->4']\n",
      "FNs: ['1->0', '4->0']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.8181818181818182\n",
      "Recall': 0.8181818181818182\n",
      "F1' score: 0.8181818181818182\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.7272727272727273\n",
      "Recall: 0.8\n",
      "F1 score: 0.761904761904762\n",
      "Percentage of delays that are correctly discovered: 75.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 145515\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim22_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 7 time steps.\n",
      "1 causes 1 with a delay of 2 time steps.\n",
      "2 causes 2 with a delay of 5 time steps.\n",
      "3 causes 3 with a delay of 30 time steps.\n",
      "4 causes 4 with a delay of 31 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 5\n",
      "Total False Negatives: 5\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 5\n",
      "TPs': ['0->0', '1->1', '2->2', '3->3', '4->4']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->2', '3->3', '4->4']\n",
      "FPs direct: []\n",
      "FNs: ['1->0', '4->0', '2->1', '3->2', '4->3']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.5\n",
      "F1' score: 0.6666666666666666\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.5\n",
      "F1 score: 0.6666666666666666\n",
      "Percentage of delays that are correctly discovered: 0.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 145515\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim23_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 1 time steps.\n",
      "1 causes 1 with a delay of 1 time steps.\n",
      "2 causes 2 with a delay of 1 time steps.\n",
      "3 causes 3 with a delay of 1 time steps.\n",
      "4 causes 4 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 0\n",
      "Total True Positives': 5\n",
      "Total False Negatives: 5\n",
      "Total Direct False Positives: 0\n",
      "Total Direct True Positives: 5\n",
      "TPs': ['0->0', '1->1', '2->2', '3->3', '4->4']\n",
      "FPs': []\n",
      "TPs direct: ['0->0', '1->1', '2->2', '3->3', '4->4']\n",
      "FPs direct: []\n",
      "FNs: ['1->0', '4->0', '2->1', '3->2', '4->3']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 1.0\n",
      "Recall': 0.5\n",
      "F1' score: 0.6666666666666666\n",
      "(includes only direct causal relationships)\n",
      "Precision: 1.0\n",
      "Recall: 0.5\n",
      "F1 score: 0.6666666666666666\n",
      "Percentage of delays that are correctly discovered: 100.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 145515\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim24_gt_processed.csv\n",
      "===================Results===================\n",
      "4 causes 0 with a delay of 1 time steps.\n",
      "4 causes 1 with a delay of 17 time steps.\n",
      "4 causes 2 with a delay of 1 time steps.\n",
      "4 causes 3 with a delay of 22 time steps.\n",
      "2 causes 4 with a delay of 1 time steps.\n",
      "4 causes 4 with a delay of 1 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 2\n",
      "Total True Positives': 4\n",
      "Total False Negatives: 7\n",
      "Total Direct False Positives: 3\n",
      "Total Direct True Positives: 3\n",
      "TPs': ['4->0', '4->2', '4->3', '4->4']\n",
      "FPs': ['4->1', '2->4']\n",
      "TPs direct: ['4->0', '4->3', '4->4']\n",
      "FPs direct: ['4->1', '4->2', '2->4']\n",
      "FNs: ['0->0', '1->0', '1->1', '2->1', '2->2', '3->2', '3->3']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.6666666666666666\n",
      "Recall': 0.36363636363636365\n",
      "F1' score: 0.4705882352941177\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.5\n",
      "Recall: 0.3\n",
      "F1 score: 0.37499999999999994\n",
      "Percentage of delays that are correctly discovered: 66.66666666666666%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 145515\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim25_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 16 time steps.\n",
      "0 causes 1 with a delay of 4 time steps.\n",
      "1 causes 1 with a delay of 30 time steps.\n",
      "2 causes 2 with a delay of 23 time steps.\n",
      "3 causes 2 with a delay of 5 time steps.\n",
      "3 causes 3 with a delay of 13 time steps.\n",
      "1 causes 4 with a delay of 30 time steps.\n",
      "2 causes 4 with a delay of 11 time steps.\n",
      "4 causes 4 with a delay of 5 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 3\n",
      "Total True Positives': 6\n",
      "Total False Negatives: 4\n",
      "Total Direct False Positives: 3\n",
      "Total Direct True Positives: 6\n",
      "TPs': ['0->0', '1->1', '2->2', '3->2', '3->3', '4->4']\n",
      "FPs': ['0->1', '1->4', '2->4']\n",
      "TPs direct: ['0->0', '1->1', '2->2', '3->2', '3->3', '4->4']\n",
      "FPs direct: ['0->1', '1->4', '2->4']\n",
      "FNs: ['1->0', '4->0', '2->1', '4->3']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.6666666666666666\n",
      "Recall': 0.6\n",
      "F1' score: 0.631578947368421\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.6\n",
      "F1 score: 0.631578947368421\n",
      "Percentage of delays that are correctly discovered: 0.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 145515\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim26_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 19 time steps.\n",
      "4 causes 0 with a delay of 18 time steps.\n",
      "1 causes 1 with a delay of 6 time steps.\n",
      "1 causes 2 with a delay of 30 time steps.\n",
      "4 causes 2 with a delay of 5 time steps.\n",
      "2 causes 3 with a delay of 14 time steps.\n",
      "4 causes 3 with a delay of 21 time steps.\n",
      "1 causes 4 with a delay of 18 time steps.\n",
      "4 causes 4 with a delay of 21 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 3\n",
      "Total True Positives': 6\n",
      "Total False Negatives: 5\n",
      "Total Direct False Positives: 4\n",
      "Total Direct True Positives: 5\n",
      "TPs': ['0->0', '4->0', '1->1', '4->2', '4->3', '4->4']\n",
      "FPs': ['1->2', '2->3', '1->4']\n",
      "TPs direct: ['0->0', '4->0', '1->1', '4->3', '4->4']\n",
      "FPs direct: ['1->2', '4->2', '2->3', '1->4']\n",
      "FNs: ['1->0', '2->1', '2->2', '3->2', '3->3']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.6666666666666666\n",
      "Recall': 0.5454545454545454\n",
      "F1' score: 0.6\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.5555555555555556\n",
      "Recall: 0.5\n",
      "F1 score: 0.5263157894736842\n",
      "Percentage of delays that are correctly discovered: 0.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 145515\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim27_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 1 time steps.\n",
      "1 causes 0 with a delay of 22 time steps.\n",
      "0 causes 1 with a delay of 6 time steps.\n",
      "1 causes 1 with a delay of 6 time steps.\n",
      "1 causes 2 with a delay of 18 time steps.\n",
      "4 causes 2 with a delay of 21 time steps.\n",
      "1 causes 3 with a delay of 1 time steps.\n",
      "4 causes 3 with a delay of 4 time steps.\n",
      "1 causes 4 with a delay of 22 time steps.\n",
      "4 causes 4 with a delay of 21 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 4\n",
      "Total True Positives': 6\n",
      "Total False Negatives: 5\n",
      "Total Direct False Positives: 5\n",
      "Total Direct True Positives: 5\n",
      "TPs': ['0->0', '1->0', '1->1', '4->2', '4->3', '4->4']\n",
      "FPs': ['0->1', '1->2', '1->3', '1->4']\n",
      "TPs direct: ['0->0', '1->0', '1->1', '4->3', '4->4']\n",
      "FPs direct: ['0->1', '1->2', '4->2', '1->3', '1->4']\n",
      "FNs: ['4->0', '2->1', '2->2', '3->2', '3->3']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.6\n",
      "Recall': 0.5454545454545454\n",
      "F1' score: 0.5714285714285713\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.5\n",
      "Recall: 0.5\n",
      "F1 score: 0.5\n",
      "Percentage of delays that are correctly discovered: 20.0%\n",
      "PredictModel(\n",
      "  (encoder): Encoder(\n",
      "    (emb): Embedding(\n",
      "      (feature_emb): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop_out): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (qk): Clone()\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): MultiVariateCausalAttention(\n",
      "            (qk_mul): einsum()\n",
      "            (softmax): Softmax(dim=-1)\n",
      "            (hardmard_product): einsum()\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): CausalConv(\n",
      "            (mul): einsum()\n",
      "          )\n",
      "          (w_concat): Linear(in_features=4, out_features=1, bias=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "          (activation): LeakyReLU(negative_slope=0.01)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((32, 1), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 145515\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:59: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.total[key] += value * n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:60: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.counts[key] += n\n",
      "/home/acme/yhy/CausalFormer/utils/util.py:61: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
      "/home/acme/yhy/CausalFormer/interpret.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "===================Running===================\n",
      "ground_truth:data/fMRI/sim28_gt_processed.csv\n",
      "===================Results===================\n",
      "0 causes 0 with a delay of 4 time steps.\n",
      "4 causes 0 with a delay of 1 time steps.\n",
      "0 causes 1 with a delay of 31 time steps.\n",
      "1 causes 1 with a delay of 31 time steps.\n",
      "2 causes 2 with a delay of 23 time steps.\n",
      "3 causes 2 with a delay of 5 time steps.\n",
      "1 causes 3 with a delay of 30 time steps.\n",
      "3 causes 3 with a delay of 24 time steps.\n",
      "2 causes 4 with a delay of 30 time steps.\n",
      "4 causes 4 with a delay of 30 time steps.\n",
      "===================Evaluation===================\n",
      "Total False Positives': 3\n",
      "Total True Positives': 7\n",
      "Total False Negatives: 3\n",
      "Total Direct False Positives: 3\n",
      "Total Direct True Positives: 7\n",
      "TPs': ['0->0', '4->0', '1->1', '2->2', '3->2', '3->3', '4->4']\n",
      "FPs': ['0->1', '1->3', '2->4']\n",
      "TPs direct: ['0->0', '4->0', '1->1', '2->2', '3->2', '3->3', '4->4']\n",
      "FPs direct: ['0->1', '1->3', '2->4']\n",
      "FNs: ['1->0', '2->1', '4->3']\n",
      "(includes direct and indirect causal relationships)\n",
      "Precision': 0.7\n",
      "Recall': 0.7\n",
      "F1' score: 0.7\n",
      "(includes only direct causal relationships)\n",
      "Precision: 0.7\n",
      "Recall: 0.7\n",
      "F1 score: 0.7\n",
      "Percentage of delays that are correctly discovered: 14.285714285714285%\n",
      "===================Summary===================\n",
      "\t    Precision'   Recall'       F1'  Precision    Recall        F1       PoD\n",
      "\t1     0.818182  0.818182  0.818182   0.727273  0.800000  0.761905  0.750000\n",
      "\t2     0.636364  0.636364  0.636364   0.590909  0.619048  0.604651  0.846154\n",
      "\t3     0.666667  0.588235  0.625000   0.633333  0.575758  0.603175  0.052632\n",
      "\t4     1.000000  0.450450  0.621118   1.000000  0.450450  0.621118  0.340000\n",
      "\t5     0.625000  0.454545  0.526316   0.500000  0.400000  0.444444  1.000000\n",
      "\t6     0.923077  0.571429  0.705882   0.923077  0.571429  0.705882  0.750000\n",
      "\t7     1.000000  0.500000  0.666667   1.000000  0.500000  0.666667  0.000000\n",
      "\t8     0.833333  0.909091  0.869565   0.750000  0.900000  0.818182  0.777778\n",
      "\t9     0.600000  0.272727  0.375000   0.400000  0.200000  0.266667  0.000000\n",
      "\t10    1.000000  0.727273  0.842105   0.875000  0.700000  0.777778  0.000000\n",
      "\t11    0.750000  0.545455  0.631579   0.687500  0.523810  0.594595  1.000000\n",
      "\t12    0.650000  0.590909  0.619048   0.600000  0.571429  0.585366  1.000000\n",
      "\t13    1.000000  0.384615  0.555556   1.000000  0.384615  0.555556  0.000000\n",
      "\t14    0.700000  0.538462  0.608696   0.400000  0.400000  0.400000  0.500000\n",
      "\t15    1.000000  0.916667  0.956522   0.818182  0.900000  0.857143  0.888889\n",
      "\t16    0.636364  0.583333  0.608696   0.636364  0.583333  0.608696  0.000000\n",
      "\t17    0.615385  0.727273  0.666667   0.576923  0.714286  0.638298  0.666667\n",
      "\t18    0.727273  0.727273  0.727273   0.636364  0.700000  0.666667  0.714286\n",
      "\t19    1.000000  0.500000  0.666667   1.000000  0.500000  0.666667  0.000000\n",
      "\t20    1.000000  0.500000  0.666667   1.000000  0.500000  0.666667  0.000000\n",
      "\t21    0.818182  0.818182  0.818182   0.727273  0.800000  0.761905  0.750000\n",
      "\t22    1.000000  0.500000  0.666667   1.000000  0.500000  0.666667  0.000000\n",
      "\t23    1.000000  0.500000  0.666667   1.000000  0.500000  0.666667  1.000000\n",
      "\t24    0.666667  0.363636  0.470588   0.500000  0.300000  0.375000  0.666667\n",
      "\t25    0.666667  0.600000  0.631579   0.666667  0.600000  0.631579  0.000000\n",
      "\t26    0.666667  0.545455  0.600000   0.555556  0.500000  0.526316  0.000000\n",
      "\t27    0.600000  0.545455  0.571429   0.500000  0.500000  0.500000  0.200000\n",
      "\t28    0.700000  0.700000  0.700000   0.700000  0.700000  0.700000  0.142857\n"
     ]
    }
   ],
   "source": [
    "! python runner.py -c config/config_fMRI.json -t fMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CausalFormer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
